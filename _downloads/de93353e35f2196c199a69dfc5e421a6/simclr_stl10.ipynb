{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Self-Supervised Contrastive Learning with SimCLR\n\nFrom: https://uvadlc-notebooks.readthedocs.io\n\nIn this tutorial, we will take a closer look at self-supervised contrastive\nlearning. Self-supervised learning, or also sometimes called unsupervised\nlearning, describes the scenario where we have given input data, but no\naccompanying labels to train in a classical supervised way. However, this\ndata still contains a lot of information from which we can learn: how are\nthe images different from each other? What patterns are descriptive for\ncertain images? Can we cluster the images? To get an insight into these\nquestions, we will implement a popular, simple contrastive learning method,\nSimCLR, and apply it to the STL10 dataset.\n\n## Setup\n\nThis notebook requires some packages besides nidl. Let's first start with\nimporting our standard libraries below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom collections import OrderedDict\nfrom copy import deepcopy\n\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torchvision\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom torchvision import transforms\nfrom torchvision.datasets import STL10\n\nfrom nidl.estimators.linear import LogisticRegression\nfrom nidl.estimators.ssl import SimCLR\nfrom nidl.utils import Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define some global parameters:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "datadir = \"/tmp/simclr/data\"\ncheckpointdir = \"/tmp/simclr/saved_models\"\nnum_workers = os.cpu_count()\nnum_images = 6\ntorch.backends.cudnn.determinstic = True\ntorch.backends.cudnn.benchmark = False\ndevice = \"gpu\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As in many tutorials before, we provide pre-trained models. If you are\nrunning this notebook locally, make sure to have sufficient disk space\navailable.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "load_pretrained = True\nos.makedirs(checkpointdir, exist_ok=True)\nweights = Weights(\n    name=\"hf-hub:neurospin/simclr-resnet18-stl10\",\n    data_dir=checkpointdir,\n    filepath=\"weights-simclr-resnet18-stl10.pt\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Augmentation for Contrastive Learning\n\nTo allow efficient training, we need to prepare the data loading such that\nwe sample two different, random augmentations for each image in the batch.\nThe easiest way to do this is by creating a transformation that, when being\ncalled, applies a set of data augmentations to an image twice. This is\nimplemented in the class nidl.transforms.MultiViewsTransform.\n\nThe contrastive learning framework can easily be extended to have more\npositive examples by sampling more than two augmentations of the same\nimage. However, the most efficient training is usually obtained by using\nonly two.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nidl.transforms import MultiViewsTransform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we can look at the specific augmentations we want to apply. The choice\nof the data augmentation to use is the most crucial hyperparameter in SimCLR\nsince it directly affects how the latent space is structured, and what\npatterns might be learned from the data.\n\nOverall, for our experiments, we apply a set of 5 transformations following\nthe original SimCLR setup: random horizontal flip, crop-and-resize, color\ndistortion, random grayscale, and gaussian blur. In comparison to the\noriginal implementation, we reduce the effect of the color jitter slightly\n(0.5 instead of 0.8 for brightness, contrast, and saturation, and 0.1\ninstead of 0.2 for hue). In our experiments, this setting obtained better\nperformance and was faster and more stable to train. If, for instance, the\nbrightness scale highly varies in a dataset, the original settings can be\nmore beneficial since the model can't rely on this information anymore to\ndistinguish between images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrast_transforms = transforms.Compose(\n    [\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomResizedCrop(size=96),\n        transforms.RandomApply(\n            [\n                transforms.ColorJitter(\n                    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1\n                )\n            ],\n            p=0.8,\n        ),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.GaussianBlur(kernel_size=9),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,)),\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n\nAfter discussing the data augmentation techniques, we can now focus on the\ndataset. In this tutorial, we will use the STL10 dataset, which, similarly to\nCIFAR10, contains images of 10 classes: airplane, bird, car, cat, deer, dog,\nhorse, monkey, ship, truck. However, the images have a higher resolution,\nnamely 96 x 96 pixels, and we are only provided with 500 labeled images per\nclass. Additionally, we have a much larger set of 100,000 unlabeled images\nwhich are similar to the training images but are sampled from a wider range\nof animals and vehicles. This makes the dataset ideal to showcase the\nbenefits that self-supervised learning offers.\n\nLuckily, the STL10 dataset is provided through torchvision. Keep in mind,\nhowever, that since this dataset is relatively large and has a considerably\nhigher resolution than CIFAR10, it requires more disk space (~3GB) and takes\na bit of time to download. For our initial discussion of self-supervised\nlearning and SimCLR, we will create two data loaders with our contrastive\ntransformations above: the unlabeled_data will be used to train our model\nvia contrastive learning, and train_data_contrast will be used as a validation\nset in contrastive learning.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unlabeled_data = STL10(\n    root=datadir,\n    split=\"unlabeled\",\n    download=True,\n    transform=MultiViewsTransform(contrast_transforms, n_views=2),\n)\ntrain_data_contrast = STL10(\n    root=datadir,\n    split=\"train\",\n    download=True,\n    transform=MultiViewsTransform(contrast_transforms, n_views=2),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before starting with our implementation of SimCLR, let's look at some example\nimage pairs sampled with our augmentations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "imgs = torch.stack(\n    [img for idx in range(num_images) for img in unlabeled_data[idx][0]], dim=0\n)\nimg_grid = torchvision.utils.make_grid(\n    imgs, nrow=num_images, normalize=True, pad_value=0.9\n)\nimg_grid = img_grid.permute(1, 2, 0)\nplt.figure(figsize=(10, 5))\nplt.title(\"Augmented image examples of the STL10 dataset\")\nplt.imshow(img_grid)\nplt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And create the associated dataloaders:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 256\ntrain_loader = data.DataLoader(\n    unlabeled_data,\n    batch_size=batch_size,\n    shuffle=True,\n    drop_last=True,\n    pin_memory=True,\n    num_workers=num_workers,\n)\nval_loader = data.DataLoader(\n    train_data_contrast,\n    batch_size=batch_size,\n    shuffle=False,\n    drop_last=False,\n    pin_memory=True,\n    num_workers=num_workers,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n\nIn our experiments, we will use the common ResNet-18 architecture as f(.), and\nwe follow the original SimCLR paper setup by defining g(.) as a two-layer MLP\nwith ReLU activation in the hidden layer. Note that in the follow-up paper,\nSimCLRv2, the authors mention that larger/wider MLPs can boost the\nperformance considerably. This is why we apply an MLP with four times\nlarger hidden dimensions, but deeper MLPs showed to overfit on the given\ndataset.\n\nA common observation in contrastive learning is that the larger the batch size,\nthe better the models perform. A larger batch size allows us to compare each\nimage to more negative examples, leading to overall smoother loss gradients.\nHowever, in our case, we experienced that a batch size of 256 was sufficient\nto get good results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hidden_dim = 128\nencoder = torchvision.models.resnet18(weights=None, num_classes=4 * hidden_dim)\nlatent_size = encoder.fc.out_features\nencoder.latent_size = latent_size\nencoder.fc = nn.Identity()\n\ncallbacks = [\n    ModelCheckpoint(\n        save_weights_only=True, mode=\"max\", monitor=\"val_acc_top5\"\n    ),\n    LearningRateMonitor(logging_interval=\"epoch\"),\n]\ntrainer_params = {\n    \"default_root_dir\": checkpointdir,\n    \"accelerator\": device,\n    \"max_epochs\": 500,\n    \"callbacks\": callbacks,\n}\nmodel = SimCLR(\n    encoder,\n    hidden_dims=[encoder.latent_size, hidden_dim],\n    lr=5e-4,\n    temperature=0.07,\n    weight_decay=1e-4,\n    random_state=42,\n    **trainer_params,\n)\n\nif load_pretrained:\n    print(f\"Found pretrained model at {weights.weight_file}, loading...\")\n    weights.load_pretrained(model)\n    model.fitted_ = True\nelse:\n    model.fit(train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression\n\nAfter we have trained our model via contrastive learning, we can deploy it\non downstream tasks and see how well it performs with little data. A common\nsetup, which also verifies whether the model has learned generalized\nrepresentations, is to perform Logistic Regression on the features. In other\nwords, we learn a single, linear layer that maps the representations to a\nclass prediction. Since the base network f(.) is not changed during the\ntraining process, the model can only perform well if the representations of\nh describe all features that might be necessary for the task. Further, we do\nnot have to worry too much about overfitting since we have very few parameters\nthat are trained. Hence, we might expect that the model can perform well even\nwith very little data.\n\nFirst, let's implement a simple Logistic Regression setup for which we assume\nthat the images already have been encoded in their feature vectors. If very\nlittle data is available, it might be beneficial to dynamically encode the\nimages during training so that we can also apply data augmentations. However,\nthe way we implement it here is much more efficient and can be trained within\na few seconds. Further, using data augmentations did not show any significant\ngain in this simple setup.\n\nThe data we use is the training and test set of STL10. The training contains\n500 images per class, while the test set has 800 images per class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 64\nscale_transforms = transforms.Compose(\n    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n)\ntrain_img_data = STL10(\n    root=datadir, split=\"train\", download=True, transform=scale_transforms\n)\ntest_img_data = STL10(\n    root=datadir, split=\"test\", download=True, transform=scale_transforms\n)\nprint(\"Number of training examples:\", len(train_img_data))\nprint(\"Number of test examples:\", len(test_img_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we create a model where the encoder weights are froozen, i.e. the\noutput representations will be used as inputs to the Logistic Regression\nmodel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_classes = 10\nnew_model = nn.Sequential(\n    OrderedDict(\n        [(\"encoder\", model.f), (\"fc\", nn.Linear(latent_size, num_classes))]\n    )\n)\nnew_model.fc.weight.data.normal_(mean=0.0, std=0.01)\nnew_model.fc.bias.data.zero_()\nnew_model.requires_grad_(False)\nnew_model.fc.requires_grad_(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we train the Logistic Regression model and evaluate the model on the\ntest set every 10 epochs to allow early stopping, but the low frequency of\nthe validation ensures that we do not overfit too much on the test set.\n\nDespite the training dataset of STL10 already only having 500 labeled images\nper class, in the original  tutorial, they perform experiments with even\nsmaller datasets.\nSpecifically, they train a Logistic Regression model for datasets with only\n10, 20, 50, 100, 200, and all 500 examples per class. This gives us an\nintuition on how well the representations learned by contrastive learning\ncan be transfered to a image recognition task like this classification.\nHere, we will only train the model with all the data available:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "weights = Weights(\n    name=\"hf-hub:neurospin/linear-resnet18-stl10\",\n    data_dir=checkpointdir,\n    filepath=\"weights-linear-resnet18-stl10.pt\",\n)\ntrain_loader = data.DataLoader(\n    train_img_data,\n    batch_size=batch_size,\n    shuffle=True,\n    drop_last=False,\n    pin_memory=True,\n    num_workers=num_workers,\n)\ntest_loader = data.DataLoader(\n    test_img_data,\n    batch_size=batch_size,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers,\n)\n\ncallbacks = [\n    ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n    LearningRateMonitor(logging_interval=\"epoch\"),\n]\ntrainer_params = {\n    \"default_root_dir\": checkpointdir,\n    \"accelerator\": device,\n    \"max_epochs\": 100,\n    \"callbacks\": callbacks,\n    \"check_val_every_n_epoch\": 10,\n}\nmodel = LogisticRegression(\n    model=deepcopy(new_model),\n    num_classes=10,\n    lr=1e-3,\n    weight_decay=1e-3,\n    random_state=42,\n    **trainer_params,\n)\nif load_pretrained:\n    print(f\"Found pretrained model at {weights.weight_file}, loading...\")\n    weights.load_pretrained(model.model.fc)\n    model.fitted_ = True\nelse:\n    model.fit(train_loader)\npreds = model.predict(test_loader)\nlabels = torch.cat([batch[1] for batch in test_loader])\nprint(f\"Predictions: {preds.shape}\")\nprint(f\"Labels: {labels.shape}\")\nacc = (preds.argmax(dim=-1) == labels).float().mean()\nprint(f\"Accuracy: {100 * acc:4.2f}%\")\n\n## _pretrained_filename = os.path.join(\n##     checkpointdir, \"weights-linear-resnet18-stl10.pt\")\n## if not os.path.isfile(_pretrained_filename):\n##     torch.save(model.model.fc.state_dict(), _pretrained_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As one would expect, the classification performance improves the more data\nwe have. However, with only 10 images per class, we can already classify more\nthan 60% of the images correctly. This is quite impressive, considering that\nthe images are also higher dimensional than e.g. CIFAR10. With the full\ndataset, we achieve an accuracy of ~80%. The increase between 50 to 500\nimages per class might suggest a linear increase in performance with an\nexponentially larger dataset. However, with even more data, we could also\nfinetune f(.) in the training process, allowing for the representations to\nadapt more to the specific classification task given.\n\n## Baseline\n\nAs a baseline to our results above, we will train a standard ResNet-18\nwith random initialization on the labeled training set of STL10. The\nresults will give us an # indication of the advantages that contrastive\nlearning on unlabeled data has compared to using only supervised training.\nThe implementation of the model is straightforward since the ResNet\narchitecture is provided in the torchvision library.\n\nIt is clear that the ResNet easily overfits on the training data since\nits parameter count is more than 1000 times larger than the dataset size.\nTo make the comparison to the contrastive learning models fair, we apply\ndata augmentations similar to the ones we used before: horizontal flip,\ncrop-and-resize, grayscale, and gaussian blur. Color distortions as before\nare not used because the color distribution of an image showed to be an\nimportant feature for the classification. Hence, we observed no noticeable\nperformance gains when adding color distortions to the set of\naugmentations. Similarly, we restrict the resizing operation before\ncropping to the max. 125% of its original resolution, instead of 1250%\nas done in SimCLR. This is because, for classification, the model needs to\nrecognize the full object, while in contrastive learning, we only want to\ncheck whether two patches belong to the same image/object. Hence, the\nchosen augmentations below are overall weaker than in the contrastive\nlearning case.\n\nThe training function for the ResNet is almost identical to the Logistic\nRegression setup. Note that we allow the ResNet to perform validation\nevery 2 epochs to also check whether the model overfits strongly in the\nfirst iterations or not.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "weights = Weights(\n    name=\"hf-hub:neurospin/resnet18-stl10\",\n    data_dir=checkpointdir,\n    filepath=\"weights-resnet18-stl10.pt\",\n)\n\nbatch_size = 64\ntrain_transforms = transforms.Compose(\n    [\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomResizedCrop(size=96, scale=(0.8, 1.0)),\n        transforms.RandomGrayscale(p=0.2),\n        transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 0.5)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,)),\n    ]\n)\ntrain_img_aug_data = STL10(\n    root=datadir, split=\"train\", download=True, transform=train_transforms\n)\ntrain_loader = data.DataLoader(\n    train_img_data,\n    batch_size=batch_size,\n    shuffle=True,\n    drop_last=False,\n    pin_memory=True,\n    num_workers=num_workers,\n)\n\ncallbacks = [\n    ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n    LearningRateMonitor(logging_interval=\"epoch\"),\n]\ntrainer_params = {\n    \"default_root_dir\": checkpointdir,\n    \"accelerator\": device,\n    \"max_epochs\": 100,\n    \"callbacks\": callbacks,\n    \"check_val_every_n_epoch\": 2,\n}\nmodel = LogisticRegression(\n    model=torchvision.models.resnet18(weights=None, num_classes=10),\n    num_classes=10,\n    lr=1e-3,\n    weight_decay=2e-4,\n    random_state=42,\n    **trainer_params,\n)\nif load_pretrained:\n    print(f\"Found pretrained model at {weights.weight_file}, loading...\")\n    weights.load_pretrained(model.model)\n    model.fitted_ = True\nelse:\n    model.fit(train_loader, test_loader)\npreds = model.predict(test_loader)\nlabels = torch.cat([batch[1] for batch in test_loader])\nprint(f\"Predictions: {preds.shape}\")\nprint(f\"Labels: {labels.shape}\")\nacc = (preds.argmax(dim=-1) == labels).float().mean()\nprint(f\"Accuracy: {100 * acc:4.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ResNet trained from scratch achieves ~73% on the test set. This\nis almost 7% less than the contrastive learning model, and even\nslightly less than SimCLR achieves with 1/10 of the data. This shows\nthat self-supervised, contrastive learning provides considerable\nperformance gains by leveraging large amounts of unlabeled data when\nlittle labeled data is available.\n\n## Conclusion\n\nIn this tutorial, we have discussed self-supervised contrastive learning\nand implemented SimCLR as an example method. We have applied it to the\nSTL10 dataset and showed that it can learn generalizable representations\nthat we can use to train simple classification models. With 500 images per\nlabel, it achieved an 8% higher accuracy than a similar model solely\ntrained from supervision and performs on par with it when only using a\ntenth of the labeled data. Our experimental results are limited to a single\ndataset, but recent works such as Ting Chen et al. showed similar trends\nfor larger datasets like ImageNet. Besides the discussed hyperparameters,\nthe size of the model seems to be important in contrastive learning as\nwell. If a lot of unlabeled data is available, larger models can achieve\nmuch stronger results and come close to their supervised baselines.\nFurther, there are also approaches for combining contrastive and\nsupervised learning, leading to performance gains beyond\nsupervision (see Khosla et al.). Moreover, contrastive learning is not\nthe only approach to self-supervised learning that has come up in the\nlast two years and showed great results. Other methods include\ndistillation-based methods like BYOL and redundancy reduction techniques\nlike Barlow Twins. There is a lot more to explore in the self-supervised\ndomain, and more, impressive steps ahead are to be expected.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}