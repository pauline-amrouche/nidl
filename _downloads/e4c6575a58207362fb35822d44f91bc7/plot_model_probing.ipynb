{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Model probing callback of embedding estimators\n\nThis notebook will show you how to investigate the **data representation given\nby an embedding estimator during training**  (such as SimCLR, y-Aware\nContrastive Learning or Barlow Twins) using the notion of \"probing\".\nA standard machine learning model (e.g. linear or SVM) is trained and evaluated\non the data embedding for a given task as the model is being fitted. It allows\nthe user to understand what concepts are learned by the model.\n\nThis has been first introduced by Guillaume Alain and Yoshua Bengio in 2017\n[1]_ to understand the internal behavior of a deep neural network along\nthe different layers. This technique aimed at answering questions like: what is\nthe intermediate representation of a neural network? What information is\ncontained for a given layer ?\n\nThen, it has been adapted to benchmark self-supervised vision models\n(like SimCLR, Barlow Twins, DINO, DINOv2) on classical datasets (ImageNet,\nCIFAR, ...) by implementing linear probing and K-Nearest Neighbors probing\non the ouput representation of the models.\n\n.. [1] Guillaume Alain and Yoshua Bengio, *Understanding intermediate layers\n       using linear classifier probes*, ICLR 2017 Workshop.\n\n## Setup\n\nThis notebook requires some packages besides nidl. Let's first start with\nimporting our standard libraries below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport re\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn.functional as func\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom tensorboard.backend.event_processing import event_accumulator\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.ops import MLP\nfrom torchvision.utils import make_grid\n\nfrom nidl.callbacks.model_probing import ClassificationProbingCallback\nfrom nidl.callbacks.multitask_probing import MultitaskModelProbing\nfrom nidl.datasets import OpenBHB\nfrom nidl.estimators.ssl import SimCLR, YAwareContrastiveLearning\nfrom nidl.transforms import MultiViewsTransform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define some global parameters that will be used throughout the notebook:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_dir = \"/tmp/mnist\"\nbatch_size = 128\nnum_workers = 10\nlatent_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unsupervised Contrastive Learning on MNIST\n\nFor illustration purposes on how to use the probing callback, we will focus\non the handwritten digits dataset MNIST. It contains 60k training images and\n10k test images of size 28x28 pixels. Each image contains a digit from 0 to\n9. It is rather small-scale compared to modern datasets like ImageNet but\nsufficient to illustrate the probing technique.\nWe will train a SimCLR model on these data and probe the learned\nrepresentation using a logistic regression classifier on the digit\nclassification task. It will show how the data embedding evolves during\ntraining to become more linearly separable for each digit class.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by loading the MNIST dataset dataset with standard scaling\ntransforms. These datasets are used for training and testing the probing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scale_transforms = transforms.Compose(\n    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n)\ntrain_xy_dataset = MNIST(data_dir, download=True, transform=scale_transforms)\ntest_xy_dataset = MNIST(\n    data_dir, download=True, train=False, transform=scale_transforms\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and data augmentations for contrastive learning\n\nTo perform contrastive learning, we need to define a set of data\naugmentations to create multiple views of the same image. Since we work\nwith grayscale images, we will use random resized crop and Gaussian blur. We\nreduce the size of the Gaussian kernel to 3x3 since MNIST images are only\n28x28 pixels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrast_transforms = transforms.Compose(\n    [\n        transforms.RandomResizedCrop(size=28),\n        transforms.GaussianBlur(kernel_size=3),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,)),\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a custom dataset that returns only the images (without labels).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SSLMNIST(MNIST):\n    def __getitem__(self, index):\n        img, _ = super().__getitem__(index)\n        return img\n\n\nssl_dataset = SSLMNIST(\n    data_dir,\n    download=True,\n    transform=MultiViewsTransform(contrast_transforms, n_views=2),\n)\ntest_ssl_dataset = SSLMNIST(\n    data_dir,\n    download=True,\n    train=False,\n    transform=MultiViewsTransform(contrast_transforms, n_views=2),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally we create the data loaders for training and testing the models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_xy_loader = DataLoader(\n    train_xy_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    drop_last=False,\n    pin_memory=True,\n    num_workers=num_workers,\n)\ntest_xy_loader = DataLoader(\n    test_xy_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers,\n)\ntrain_ssl_loader = DataLoader(\n    ssl_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    pin_memory=True,\n    num_workers=num_workers,\n)\ntest_ssl_loader = DataLoader(\n    test_ssl_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    pin_memory=True,\n    num_workers=num_workers,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before starting training the SimCLR model, let's visualize some\nexamples of the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def show_images(images, title=None, nrow=8):\n    grid = make_grid(images, nrow=nrow, normalize=True, pad_value=1)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(grid.permute(1, 2, 0).cpu())\n    if title:\n        plt.title(title)\n    plt.axis(\"off\")\n    plt.show()\n\n\n# Original and augmented images\nimages, labels = next(iter(test_xy_loader))\naugmented_views = next(iter(test_ssl_loader))\nview1, view2 = augmented_views[0], augmented_views[1]\nfig, axes = plt.subplots(2, 3, figsize=(6, 4))\nfor i in range(2):\n    axes[i, 0].imshow(images[i][0].cpu(), cmap=\"gray\")\n    axes[i, 0].set_title(f\"Original (label={labels[i].item()})\")\n    axes[i, 0].axis(\"off\")\n\n    axes[i, 1].imshow(view1[i][0].cpu(), cmap=\"gray\")\n    axes[i, 1].set_title(\"Augmented View 1\")\n    axes[i, 1].axis(\"off\")\n\n    axes[i, 2].imshow(view2[i][0].cpu(), cmap=\"gray\")\n    axes[i, 2].set_title(\"Augmented View 2\")\n    axes[i, 2].axis(\"off\")\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SimCLR training with classification probing callback\n\nWe can now create the probing callback that will train a logistic regression\nclassifier on the learned representation during SimCLR training. The probing\nis performed every 2 epochs on the training and test sets. The classification\nmetrics are logged to TensorBoard by default.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "callback = ClassificationProbingCallback(\n    train_xy_loader,\n    test_xy_loader,\n    probe=LogisticRegression(max_iter=200),\n    every_n_train_epochs=2,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since MNIST images are small, we can use a simple LeNet-like architecture\nas encoder for SimCLR, with few parameters. The output dimension of the\nencoder is set to 32, which is approximately 30 times smaller that the input,\nbut larger than the number of input classes (10).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class LeNetEncoder(nn.Module):\n    def __init__(self, latent_size=32):\n        super().__init__()\n        self.latent_size = latent_size\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n        self.pool1 = nn.AvgPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.pool2 = nn.AvgPool2d(2, 2)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, latent_size)\n\n    def forward(self, x):\n        x = func.relu(self.conv1(x))\n        x = self.pool1(x)\n        x = func.relu(self.conv2(x))\n        x = self.pool2(x)\n        x = x.view(x.size(0), -1)\n        x = func.relu(self.fc1(x))\n        x = func.relu(self.fc2(x))\n        return self.fc3(x)\n\n\nencoder = LeNetEncoder(latent_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now create the SimCLR model with the encoder and the probing callback.\nWe limit the training to 10 epochs for the sake of time and because it is\nenough for checking the evolution of the embedding geometry across training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = SimCLR(\n    encoder=encoder,\n    random_state=42,\n    limit_train_batches=100,\n    max_epochs=10,\n    temperature=0.1,\n    hidden_dims=[64, 32],\n    lr=1e-4,\n    weight_decay=1e-4,\n    enable_checkpointing=False,\n    callbacks=callback,  # <-- key part for probing\n)\nmodel.fit(train_ssl_loader, test_ssl_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization of the classification metrics during training\n\nAfter training, we can visualize the classification metrics logged\nby the linear probe using TensorBoard. The logged metrics are stored\nin the `lightning_logs` folder by default. They contain the accuracy,\nbalanced accuracy, F1-score (weighted and macro), precision (macro)\nand recall (macro).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_last_log_version(logs_dir=\"lightning_logs\"):\n    versions = []\n    for d in os.listdir(logs_dir):\n        match = re.match(r\"version_(\\d+)\", d)\n        if match:\n            versions.append(int(match.group(1)))\n    return max(versions) if versions else None\n\n\nlog_dir = f\"lightning_logs/version_{get_last_log_version()}/\"\nea = event_accumulator.EventAccumulator(log_dir)\nea.Reload()\nmetrics = [\n    \"LogisticRegression/accuracy\",\n    \"LogisticRegression/balanced_accuracy\",\n    \"LogisticRegression/f1_weighted\",\n    \"LogisticRegression/f1_macro\",\n    \"LogisticRegression/precision_macro\",\n    \"LogisticRegression/recall_macro\",\n]\nscalars = {\n    m.replace(\"LogisticRegression/\", \"\"): ea.Scalars(m) for m in metrics\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once all the metrics are loaded, we plot them as the number of training steps\nincreases:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 3))\nfor m, events in scalars.items():\n    steps = [e.step for e in events]\n    values = [e.value for e in events]\n    plt.plot(steps, values, label=m)\nplt.xlabel(f\"Nb steps (batch size={batch_size})\")\nplt.ylabel(\"Metric score\")\nplt.title(\"Classification metrics during SimCLR training\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observations**: we can see that the classification metrics increase\nsteadily during training, showing that the learned representation becomes\nmore and more linearly separable for the digit classes. The accuracy\nreaches more than 80% after 10 epochs, which is quite good for such a simple\nmodel trained *without supervision* and a small number of epochs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Probing of y-Aware representation on age and sex prediction\n\nWe have previously seen a simple case where only one classification task is\nbeing monitored during training. We can also monitor a mixed of classification\nand regression tasks at the same time during training of an embedding model.\nThis could be useful if several target variables should be monitored from the\nrepresentation.\nWe will show how to perform this with NIDL using the **MultitaskModelProbing**\ncallback on the OpenBHB dataset to monitor age and sex decoding from brain\nimaging data. *We refer to the example on OpenBHB for more details on this\nneuroimaging dataset.*\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the relevant global parameters for this example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_dir = \"/tmp/openbhb\"\nbatch_size = 128\nnum_workers = 10\nlatent_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenBHB dataset and data augmentations\n\nWe consider the gray matter and CSF volumes on some **regions of\ninterests** in the Neuromorphometrics atlas across subjects in\nOpenBHB (\"vbm_roi\" modality). These data are tabular (not images) but\nthey are still well suited for contrastive learning and they are very light\ncompared to the raw images (284-d vector for each subject).\nWe start by loading these data for training and testing the probing callback.\nThe target variables are age (regression) and sex (classification).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def target_transforms(labels):\n    return np.array([labels[\"age\"], labels[\"sex\"] == \"female\"])\n\n\ntrain_xy_dataset = OpenBHB(\n    data_dir,\n    modality=\"vbm_roi\",\n    target=[\"age\", \"sex\"],\n    transforms=lambda x: x.flatten(),\n    target_transforms=target_transforms,\n    streaming=False,\n)\ntest_xy_dataset = OpenBHB(\n    data_dir,\n    modality=\"vbm_roi\",\n    split=\"val\",\n    target=[\"age\", \"sex\"],\n    transforms=lambda x: x.flatten(),\n    target_transforms=target_transforms,\n    streaming=False,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform contrastive learning, we will use random masking and Gaussian\nnoise as data augmentations. These are well suited for tabular data.\nWe will train a **y-Aware Contrastive Learning** model on these data, using\n**age as auxiliary variable**.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask_prob = 0.8\nnoise_std = 0.5\ncontrast_transforms = transforms.Compose(\n    [\n        lambda x: x.flatten(),\n        lambda x: (np.random.rand(*x.shape) > mask_prob).astype(np.float32)\n        * x,  # random masking\n        lambda x: x\n        + (\n            (np.random.rand() > 0.5) * np.random.randn(*x.shape) * noise_std\n        ).astype(np.float32),  # random Gaussian noise\n    ]\n)\n\nssl_dataset = OpenBHB(\n    data_dir,\n    modality=\"vbm_roi\",\n    target=\"age\",\n    transforms=MultiViewsTransform(contrast_transforms, n_views=2),\n)\ntest_ssl_dataset = OpenBHB(\n    data_dir,\n    modality=\"vbm_roi\",\n    target=\"age\",\n    split=\"val\",\n    transforms=MultiViewsTransform(contrast_transforms, n_views=2),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As before, we create the data loaders for training and testing the models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_xy_loader = DataLoader(\n    train_xy_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    drop_last=False,\n    pin_memory=True,\n    num_workers=num_workers,\n)\ntest_xy_loader = DataLoader(\n    test_xy_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    drop_last=False,\n    num_workers=num_workers,\n)\ntrain_ssl_loader = DataLoader(\n    ssl_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    pin_memory=True,\n    num_workers=num_workers,\n)\ntest_ssl_loader = DataLoader(\n    test_ssl_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    pin_memory=True,\n    num_workers=num_workers,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## y-Aware CL training with multitask probing callback\n\nWe can now create the multitask probing callback that will train a ridge\nregression on age and a logistic regression classifier on sex. The probing\nis performed every epoch on the training and test sets. The metrics are\nlogged to TensorBoard by default.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "callback = MultitaskModelProbing(\n    train_xy_loader,\n    test_xy_loader,\n    probes=[Ridge(), LogisticRegression(max_iter=200)],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we work with tabular data, we can use a simple MLP as encoder for\ny-Aware Contrastive Learning. The input dimension is 284 and we compress the\ndata to a 32-d latent space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "encoder = MLP(in_channels=284, hidden_channels=[64, latent_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now create the y-Aware Contrastive Learning model with the MLP encoder\nand the multitask probing callback. We limit the training to 10 epochs for\nthe sake of time and we use a small bandwidth for the Gaussian kernel in the\ny-Aware model compared to the variance of the age in OpenBHB (sigma=4).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma = 4\nmodel = YAwareContrastiveLearning(\n    encoder=encoder,\n    projection_head_kwargs={\n        \"input_dim\": latent_size,\n        \"hidden_dim\": 2 * latent_size,\n        \"output_dim\": latent_size,\n    },\n    bandwidth=sigma**2,\n    random_state=42,\n    max_epochs=10,\n    temperature=0.1,\n    learning_rate=1e-5,\n    enable_checkpointing=False,\n    callbacks=callback,  # <-- add callback to monitor the training\n)\n\nmodel.fit(train_ssl_loader, test_ssl_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization of the classification and regression metrics during training\n\nAfter training, we can visualize the classification and regression metrics\nlogged by the multitask probing using TensorBoard. The logged metrics are\nstored in the `lightning_logs` folder by default. They contain the R2 score\n(coefficient of determination), the explained variance (EVar), the Pearson\nCorrelation Coefficient (PCC) for age regression and the accuracy, balanced\naccuracy, F1-score (weighted and macro), precision (macro) and recall (macro)\nfor sex classification.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_last_log_version(logs_dir=\"lightning_logs\"):\n    \"\"\"Return the last Lightning log version as an integer.\"\"\"\n    versions = []\n    for d in os.listdir(logs_dir):\n        match = re.match(r\"version_(\\d+)\", d)\n        if match:\n            versions.append(int(match.group(1)))\n    return max(versions) if versions else None\n\n\nlog_dir = f\"lightning_logs/version_{get_last_log_version()}/\"\n\n# Reload the log file\nea = event_accumulator.EventAccumulator(log_dir)\nea.Reload()\nmetrics = [\n    \"task0/R2\",\n    \"task0/PCC\",  # Pearson Correlaction Coefficient\n    \"task0/EVar\",\n    \"task1/accuracy\",\n    \"task1/balanced_accuracy\",\n    \"task1/f1_macro\",\n    \"task1/precision_macro\",\n    \"task1/recall_macro\",\n    \"task1/f1_weighted\",\n]\n# fetch all events\nscalars = {m: ea.Scalars(m) for m in metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once all the metrics are loaded, we plot them as the number of training steps\nincreases. We create two subplots, one for each task (age regression and sex\nclassification).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_task(ax, task_prefix, title):\n    task_metrics = [m for m in metrics if m.startswith(task_prefix)]\n    for m in task_metrics:\n        steps = [s.step for s in scalars[m]]\n        values = [s.value for s in scalars[m]]\n        ax.plot(steps, values, label=m.split(\"/\")[1])\n    ax.set_title(title)\n    ax.set_xlabel(\"Step\")\n    ax.set_ylabel(\"Metric Value\")\n    ax.legend()\n    ax.grid(True)\n\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nplot_task(axes[0], \"task0\", \"Task 0: Age Regression\")\nplot_task(axes[1], \"task1\", \"Task 1: Sex Classification\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions\n\nIn this notebook, we have shown how to use the model probing callbacks\navailable in NIDL to monitor the evolution of the data representation\nduring training of embedding models such as SimCLR and y-Aware Contrastive\nLearning. We have seen how to use the `ClassificationProbingCallback` for\nsingle-task probing and the `MultitaskModelProbing` for multi-task probing.\nThese callbacks allow to train standard machine learning models (e.g.\nlogistic regression, ridge regression, SVM) on the learned representation\nat regular intervals during training and log the relevant metrics to\nTensorBoard. This provides insights on what concepts are being learned by\nthe model and how the representation evolves to become more suitable for\ndownstream tasks.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}