<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Weakly Supervised Contrastive Learning with y-Aware" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://neurospin-deepinsight.github.io/auto_examples/plot_yaware_openbhb.html" />
<meta property="og:site_name" content="Nidl" />
<meta property="og:description" content="This tutorial will show you how to fit and evaluate a y-Aware Contrastive Learning model on the OpenBHB dataset using NIDL. As in the original paper 1, we will use age as a weak label to guide the ..." />
<meta property="og:image" content="https://neurospin-deepinsight.github.io/nidl/_static/nidl-logo.png" />
<meta property="og:image:alt" content="Nidl" />
<meta name="description" content="This tutorial will show you how to fit and evaluate a y-Aware Contrastive Learning model on the OpenBHB dataset using NIDL. As in the original paper 1, we will use age as a weak label to guide the ..." />
<link rel="search" title="Search" href="../search.html"><link rel="next" title="User guide" href="../user_guide.html"><link rel="prev" title="Self-Supervised Learning with Barlow Twins" href="plot_barlowtwins_openbhb.html">
        <link rel="prefetch" href="../_static/nidl-transparent.png" as="image">

    <link rel="shortcut icon" href="../_static/favicon.ico"><!-- Generated with Sphinx 8.2.3 and Furo 2025.09.25 -->
        <title>Weakly Supervised Contrastive Learning with y-Aware - Nidl</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=2da93098" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=580074bf" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=749372d1" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/fontawesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/solid.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/brands.min.css" />
    
    


<style>
  body {
    --color-code-background: #ffffff;
  --color-code-foreground: black;
  --admonition-font-size: 100%;
  --admonition-title-font-size: 100%;
  --color-announcement-background: #FBB360;
  --color-announcement-text: #111418;
  --color-admonition-title--note: #448aff;
  --color-admonition-title-background--note: #448aff10;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  --color-announcement-background: #935610;
  --color-announcement-text: #FFFFFF;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  --color-announcement-background: #935610;
  --color-announcement-text: #FFFFFF;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>

<div class="announcement">
  <aside class="announcement-content">
     <p>This is the development documentation of nidl (0.0.1)  
  </aside>
</div>

<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Nidl</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../_static/nidl-transparent.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Nidl</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Examples</a><input aria-label="Toggle navigation of Examples" checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="plot_model_probing.html">Model probing callback of embedding estimators</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_openbhb.html">Presentation of the OpenBHB dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="simclr_stl10.html">Self-Supervised Contrastive Learning with SimCLR</a></li>
<li class="toctree-l2"><a class="reference internal" href="plot_barlowtwins_openbhb.html">Self-Supervised Learning with Barlow Twins</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Weakly Supervised Contrastive Learning with y-Aware</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../user_guide.html">User guide</a><input aria-label="Toggle navigation of User guide" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../introduction.html">1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../introduction.html#what-is-nidl">2. What is <code class="docutils literal notranslate"><span class="pre">nidl</span></code>?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../introduction.html#using-nidl-for-the-first-time">3. Using <code class="docutils literal notranslate"><span class="pre">nidl</span></code> for the first time</a></li>
<li class="toctree-l2"><a class="reference internal" href="../introduction.html#applications-to-neuroimaging">4. Applications to Neuroimaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../supervised_learning/index.html">5. Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../self_supervised_learning/index.html">6. Self Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoencoders/index.html">7. Auto Encoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_probing.html">8. Model Probing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_augmentation/index.html">9. Data Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pretrained_models.html">10. Pretrained Models</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../architectures/index.html">11. Architectures</a><input aria-label="Toggle navigation of 11. Architectures" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../architectures/volume.html">11.1. Volume</a></li>
<li class="toctree-l3"><a class="reference internal" href="../architectures/surface.html">11.2. Surface</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../open_datasets.html">12. Open Datasets</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../modules/index.html">API References</a><input aria-label="Toggle navigation of API References" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../modules/estimators.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.estimators</span></code>: Available estimators</a><input aria-label="Toggle navigation of nidl.estimators: Available estimators" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.estimators.BaseEstimator.html">nidl.estimators.BaseEstimator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.estimators.ClassifierMixin.html">nidl.estimators.ClassifierMixin</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.estimators.ClusterMixin.html">nidl.estimators.ClusterMixin</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.estimators.RegressorMixin.html">nidl.estimators.RegressorMixin</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.estimators.TransformerMixin.html">nidl.estimators.TransformerMixin</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.estimators.ssl.SimCLR.html">nidl.estimators.ssl.SimCLR</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.estimators.ssl.YAwareContrastiveLearning.html">nidl.estimators.ssl.YAwareContrastiveLearning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.estimators.ssl.BarlowTwins.html">nidl.estimators.ssl.BarlowTwins</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.losses.InfoNCE.html">nidl.losses.InfoNCE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.losses.YAwareInfoNCE.html">nidl.losses.YAwareInfoNCE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.losses.BarlowTwinsLoss.html">nidl.losses.BarlowTwinsLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.estimators.ssl.utils.ProjectionHead.html">nidl.estimators.ssl.utils.ProjectionHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.estimators.ssl.utils.SimCLRProjectionHead.html">nidl.estimators.ssl.utils.SimCLRProjectionHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.estimators.ssl.utils.YAwareProjectionHead.html">nidl.estimators.ssl.utils.YAwareProjectionHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.estimators.ssl.utils.BarlowTwinsProjectionHead.html">nidl.estimators.ssl.utils.BarlowTwinsProjectionHead</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.estimators.autoencoders.VAE.html">nidl.estimators.autoencoders.VAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.losses.BetaVAELoss.html">nidl.losses.BetaVAELoss</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../modules/architectures.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.volume.backbones</span></code>: Available backbones</a><input aria-label="Toggle navigation of nidl.volume.backbones: Available backbones" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.utils.Weights.html">nidl.utils.Weights</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.backbones.AlexNet.html">nidl.volume.backbones.AlexNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.backbones.DenseNet.html">nidl.volume.backbones.DenseNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.backbones.ResNet.html">nidl.volume.backbones.ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.backbones.ResNetTruncated.html">nidl.volume.backbones.ResNetTruncated</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.backbones.densenet121.html">nidl.volume.backbones.densenet121</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.backbones.resnet18_trunc.html">nidl.volume.backbones.resnet18_trunc</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.backbones.resnet50.html">nidl.volume.backbones.resnet50</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.backbones.resnet50_trunc.html">nidl.volume.backbones.resnet50_trunc</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../modules/augmentation.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.transforms</span></code>: Available transformations</a><input aria-label="Toggle navigation of nidl.transforms: Available transformations" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.transforms.Transform.html">nidl.transforms.Transform</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.transforms.Identity.html">nidl.transforms.Identity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.transforms.MultiViewsTransform.html">nidl.transforms.MultiViewsTransform</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.transforms.VolumeTransform.html">nidl.transforms.VolumeTransform</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.transforms.preprocessing.RobustRescaling.html">nidl.volume.transforms.preprocessing.RobustRescaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.transforms.preprocessing.ZNormalization.html">nidl.volume.transforms.preprocessing.ZNormalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.transforms.preprocessing.CropOrPad.html">nidl.volume.transforms.preprocessing.CropOrPad</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.transforms.preprocessing.Resample.html">nidl.volume.transforms.preprocessing.Resample</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.transforms.preprocessing.Resize.html">nidl.volume.transforms.preprocessing.Resize</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.transforms.augmentation.RandomGaussianBlur.html">nidl.volume.transforms.augmentation.RandomGaussianBlur</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.transforms.augmentation.RandomGaussianNoise.html">nidl.volume.transforms.augmentation.RandomGaussianNoise</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.transforms.augmentation.RandomErasing.html">nidl.volume.transforms.augmentation.RandomErasing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.transforms.augmentation.RandomFlip.html">nidl.volume.transforms.augmentation.RandomFlip</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.transforms.augmentation.RandomResizedCrop.html">nidl.volume.transforms.augmentation.RandomResizedCrop</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.volume.transforms.augmentation.RandomRotation.html">nidl.volume.transforms.augmentation.RandomRotation</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../modules/datasets.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.datasets</span></code>: Available datasets</a><input aria-label="Toggle navigation of nidl.datasets: Available datasets" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.datasets.BaseImageDataset.html">nidl.datasets.BaseImageDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.datasets.BaseNumpyDataset.html">nidl.datasets.BaseNumpyDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.datasets.ImageDataFrameDataset.html">nidl.datasets.ImageDataFrameDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.datasets.OpenBHB.html">nidl.datasets.OpenBHB</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../modules/callbacks.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nidl.callbacks</span></code>: Available callbacks</a><input aria-label="Toggle navigation of nidl.callbacks: Available callbacks" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><span class="icon"><svg><use href="#svg-arrow-right"></use></svg></span></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.callbacks.BatchTypingCallback.html">nidl.callbacks.BatchTypingCallback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.callbacks.BatchTypingCallback.html">nidl.callbacks.BatchTypingCallback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.callbacks.ClassificationProbingCallback.html">nidl.callbacks.ClassificationProbingCallback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.callbacks.RegressionProbingCallback.html">nidl.callbacks.RegressionProbingCallback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.callbacks.MultitaskModelProbing.html">nidl.callbacks.MultitaskModelProbing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../modules/generated/nidl.callbacks.ModelProbing.html">nidl.callbacks.ModelProbing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../development.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ci.html">Continuous integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../maintenance.html">Maintenance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../whats_new.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../authors.html">Team</a></li>
<li class="toctree-l1"><a class="reference internal" href="../versions.html">Versions</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neurospin-deepinsight/nidl">GitHub Repository</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="https://github.com/neurospin-deepinsight/nidl/blob/main/doc/auto_examples/plot_yaware_openbhb.rst?plain=true" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div><div class="edit-this-page">
  <a class="muted-link" href="https://github.com/neurospin-deepinsight/nidl/edit/main/doc/auto_examples/plot_yaware_openbhb.rst" rel="edit" title="Edit this page">
    <svg><use href="#svg-pencil"></use></svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-examples-plot-yaware-openbhb-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="weakly-supervised-contrastive-learning-with-y-aware">
<span id="sphx-glr-auto-examples-plot-yaware-openbhb-py"></span><h1>Weakly Supervised Contrastive Learning with y-Aware<a class="headerlink" href="#weakly-supervised-contrastive-learning-with-y-aware" title="Link to this heading">¶</a></h1>
<p>This tutorial will show you how to fit and evaluate a y-Aware Contrastive
Learning model on the OpenBHB dataset using NIDL. As in the original paper
<a class="footnote-reference brackets" href="#id2" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, we will use age as a weak label to guide the contrastive learning
process. The model will be trained to bring representations of samples with
similar ages closer together in the feature space, while pushing apart samples
with dissimilar ages. This approach leverages the age information to enhance
the quality of the learned representations, making them more relevant for
downstream tasks such as age prediction or disease classification.</p>
<p>We will follow these steps using the NIDL library:</p>
<ol class="arabic simple">
<li><p>Load the OpenBHB dataset.</p></li>
<li><p>Define the data augmentations for weakly-supervised training.</p></li>
<li><p>Define the y-Aware Contrastive Learning model.</p></li>
<li><p>Train the model using the weak labels.</p></li>
<li><p>Visualize the model’s embedding using MDS and evaluate its
performance on age prediction using linear regression and KNN.</p></li>
</ol>
<p>As for the neuroimaging data, we will investigate two input representations:</p>
<ul>
<li><p>Voxel-based morphometry (VBM) maps, which are preprocessed gray matter
density maps.</p></li>
<li><p>Surface-based morphometry (SBM) maps, which are cortical thickness, mean
curvature, gray matter volume and surface area maps projected onto a
standard surface template.</p>
<p>Both representations are available in the OpenBHB dataset. To make the
training faster and reduce the memory footprint, we will consider regions
of interest (ROIs) instead of the whole brain. For VBM, we will
use the mean gray matter density averaged within each ROI of the
Neuromorphometrics atlas (284 regions). For SBM, we will use the cortical
thickness, mean curvature, gray matter volume and surface area averaged
within each ROI of the Desikan-Killiany atlas (68 regions).</p>
<p>The y-Aware Contrastive Learning model will be trained individually on both
representations and we will compare their performance on age prediction.</p>
</li>
</ul>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Dufumier et al., Contrastive learning with continuous proxy meta-data
for 3d mri classification, MICCAI 2021.</p>
</aside>
</aside>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">¶</a></h2>
<p>This notebook requires some packages besides nidl. Let’s first start with
importing our standard libraries below:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">transforms</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-class"><span class="n">LinearRegression</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.manifold</span><span class="w"> </span><span class="kn">import</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS" class="sphx-glr-backref-module-sklearn-manifold sphx-glr-backref-type-py-class"><span class="n">MDS</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error" class="sphx-glr-backref-module-sklearn-metrics sphx-glr-backref-type-py-function"><span class="n">mean_absolute_error</span></a><span class="p">,</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score" class="sphx-glr-backref-module-sklearn-metrics sphx-glr-backref-type-py-function"><span class="n">r2_score</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neighbors</span><span class="w"> </span><span class="kn">import</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor" title="sklearn.neighbors.KNeighborsRegressor" class="sphx-glr-backref-module-sklearn-neighbors sphx-glr-backref-type-py-class"><span class="n">KNeighborsRegressor</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"><span class="n">DataLoader</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.ops</span><span class="w"> </span><span class="kn">import</span> <a href="https://docs.pytorch.org/vision/main/generated/torchvision.ops.MLP.html#torchvision.ops.MLP" title="torchvision.ops.MLP" class="sphx-glr-backref-module-torchvision-ops sphx-glr-backref-type-py-class"><span class="n">MLP</span></a>

<span class="kn">from</span><span class="w"> </span><span class="nn">nidl.datasets</span><span class="w"> </span><span class="kn">import</span> <a href="../modules/generated/nidl.datasets.OpenBHB.html#nidl.datasets.OpenBHB" title="nidl.datasets.OpenBHB" class="sphx-glr-backref-module-nidl-datasets sphx-glr-backref-type-py-class"><span class="n">OpenBHB</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">nidl.estimators.ssl</span><span class="w"> </span><span class="kn">import</span> <a href="../modules/generated/nidl.estimators.ssl.YAwareContrastiveLearning.html#nidl.estimators.ssl.YAwareContrastiveLearning" title="nidl.estimators.ssl.YAwareContrastiveLearning" class="sphx-glr-backref-module-nidl-estimators-ssl sphx-glr-backref-type-py-class"><span class="n">YAwareContrastiveLearning</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">nidl.transforms</span><span class="w"> </span><span class="kn">import</span> <a href="../modules/generated/nidl.transforms.MultiViewsTransform.html#nidl.transforms.MultiViewsTransform" title="nidl.transforms.MultiViewsTransform" class="sphx-glr-backref-module-nidl-transforms sphx-glr-backref-type-py-class"><span class="n">MultiViewsTransform</span></a>
</pre></div>
</div>
<p>We define some global parameters that will be used throughout the notebook:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_dir</span></a> <span class="o">=</span> <span class="s2">&quot;/tmp/openbhb&quot;</span>
<a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a> <span class="o">=</span> <span class="mi">128</span>
<a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a> <span class="o">=</span> <span class="mi">10</span>
<a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">latent_size</span></a> <span class="o">=</span> <span class="mi">32</span>
</pre></div>
</div>
</section>
<section id="openbhb-datasets-and-data-augmentations-for-contrastive-learning">
<h2>OpenBHB datasets and data augmentations for Contrastive Learning<a class="headerlink" href="#openbhb-datasets-and-data-augmentations-for-contrastive-learning" title="Link to this heading">¶</a></h2>
<p>We will use the OpenBHB dataset for pre-training the models. We will focus
on the VBM ROI representation and the SBM ROI representation for this
tutorial. Since they are tabular data, we will use random masking and
adding Gaussian noise as data augmentation in contrastive learning.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hyperparameters for data augmentations</span>
<a href="https://docs.python.org/3/library/functions.html#float" title="builtins.float" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">mask_prob</span></a> <span class="o">=</span> <span class="mf">0.8</span>
<a href="https://docs.python.org/3/library/functions.html#float" title="builtins.float" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">noise_std</span></a> <span class="o">=</span> <span class="mf">0.5</span>
<a href="https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose" title="torchvision.transforms.Compose" class="sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">contrast_transforms</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose" title="torchvision.transforms.Compose" class="sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span></a><span class="p">(</span>
    <span class="p">[</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html#numpy.random.rand" title="numpy.random.rand" class="sphx-glr-backref-module-numpy-random sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span></a><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <a href="https://docs.python.org/3/library/functions.html#float" title="builtins.float" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">mask_prob</span></a><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float32" title="numpy.float32" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">np</span><span class="o">.</span><span class="n">float32</span></a><span class="p">)</span>
        <span class="o">*</span> <span class="n">x</span><span class="p">,</span>  <span class="c1"># random masking</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span>
        <span class="o">+</span> <span class="p">(</span>
            <span class="p">(</span><a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html#numpy.random.rand" title="numpy.random.rand" class="sphx-glr-backref-module-numpy-random sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span></a><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html#numpy.random.randn" title="numpy.random.randn" class="sphx-glr-backref-module-numpy-random sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <a href="https://docs.python.org/3/library/functions.html#float" title="builtins.float" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">noise_std</span></a>
        <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float32" title="numpy.float32" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"><span class="n">np</span><span class="o">.</span><span class="n">float32</span></a><span class="p">),</span>  <span class="c1"># random Gaussian noise</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We first create the SSL dataloaders with VBM modality and age as weak label.
We use the previous contrastive transforms for data augmentation.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_ssl_vbm</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"><span class="n">DataLoader</span></a><span class="p">(</span>
    <a href="../modules/generated/nidl.datasets.OpenBHB.html#nidl.datasets.OpenBHB" title="nidl.datasets.OpenBHB" class="sphx-glr-backref-module-nidl-datasets sphx-glr-backref-type-py-class"><span class="n">OpenBHB</span></a><span class="p">(</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_dir</span></a><span class="p">,</span>
        <span class="n">modality</span><span class="o">=</span><span class="s2">&quot;vbm_roi&quot;</span><span class="p">,</span>
        <span class="n">target</span><span class="o">=</span><span class="s2">&quot;age&quot;</span><span class="p">,</span>
        <span class="n">streaming</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">transforms</span><span class="o">=</span><a href="../modules/generated/nidl.transforms.MultiViewsTransform.html#nidl.transforms.MultiViewsTransform" title="nidl.transforms.MultiViewsTransform" class="sphx-glr-backref-module-nidl-transforms sphx-glr-backref-type-py-class"><span class="n">MultiViewsTransform</span></a><span class="p">(</span><a href="https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose" title="torchvision.transforms.Compose" class="sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">contrast_transforms</span></a><span class="p">,</span> <span class="n">n_views</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="p">),</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_ssl_vbm_test</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"><span class="n">DataLoader</span></a><span class="p">(</span>
    <a href="../modules/generated/nidl.datasets.OpenBHB.html#nidl.datasets.OpenBHB" title="nidl.datasets.OpenBHB" class="sphx-glr-backref-module-nidl-datasets sphx-glr-backref-type-py-class"><span class="n">OpenBHB</span></a><span class="p">(</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_dir</span></a><span class="p">,</span>
        <span class="n">modality</span><span class="o">=</span><span class="s2">&quot;vbm_roi&quot;</span><span class="p">,</span>
        <span class="n">target</span><span class="o">=</span><span class="s2">&quot;age&quot;</span><span class="p">,</span>
        <span class="n">split</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">,</span>
        <span class="n">streaming</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">transforms</span><span class="o">=</span><a href="../modules/generated/nidl.transforms.MultiViewsTransform.html#nidl.transforms.MultiViewsTransform" title="nidl.transforms.MultiViewsTransform" class="sphx-glr-backref-module-nidl-transforms sphx-glr-backref-type-py-class"><span class="n">MultiViewsTransform</span></a><span class="p">(</span><a href="https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose" title="torchvision.transforms.Compose" class="sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">contrast_transforms</span></a><span class="p">,</span> <span class="n">n_views</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="p">),</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
</pre></div>
</div>
<p>Then, we create the SSL dataloaders with SBM modality on the Desikan-Killiany
atlas and age as weak label. We only extract some surface features and we use
the same contrastive transforms as for VBM.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extract only surface area, GM volume, cortical thickness, mean curvature for</span>
<span class="c1"># SBM maps</span>
<a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sbm_channels</span></a> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">sbm_transform</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sbm_channels</span></a><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>


<span class="k">def</span><span class="w"> </span><span class="nf">vbm_transform</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>


<a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_ssl_sbm</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"><span class="n">DataLoader</span></a><span class="p">(</span>
    <a href="../modules/generated/nidl.datasets.OpenBHB.html#nidl.datasets.OpenBHB" title="nidl.datasets.OpenBHB" class="sphx-glr-backref-module-nidl-datasets sphx-glr-backref-type-py-class"><span class="n">OpenBHB</span></a><span class="p">(</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_dir</span></a><span class="p">,</span>
        <span class="n">modality</span><span class="o">=</span><span class="s2">&quot;fs_desikan_roi&quot;</span><span class="p">,</span>
        <span class="n">target</span><span class="o">=</span><span class="s2">&quot;age&quot;</span><span class="p">,</span>
        <span class="n">streaming</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">transforms</span><span class="o">=</span><a href="../modules/generated/nidl.transforms.MultiViewsTransform.html#nidl.transforms.MultiViewsTransform" title="nidl.transforms.MultiViewsTransform" class="sphx-glr-backref-module-nidl-transforms sphx-glr-backref-type-py-class"><span class="n">MultiViewsTransform</span></a><span class="p">(</span>
            <a href="https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose" title="torchvision.transforms.Compose" class="sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span></a><span class="p">([</span><span class="n">sbm_transform</span><span class="p">,</span> <a href="https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose" title="torchvision.transforms.Compose" class="sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">contrast_transforms</span></a><span class="p">]),</span> <span class="n">n_views</span><span class="o">=</span><span class="mi">2</span>
        <span class="p">),</span>
    <span class="p">),</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_ssl_sbm_test</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"><span class="n">DataLoader</span></a><span class="p">(</span>
    <a href="../modules/generated/nidl.datasets.OpenBHB.html#nidl.datasets.OpenBHB" title="nidl.datasets.OpenBHB" class="sphx-glr-backref-module-nidl-datasets sphx-glr-backref-type-py-class"><span class="n">OpenBHB</span></a><span class="p">(</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_dir</span></a><span class="p">,</span>
        <span class="n">modality</span><span class="o">=</span><span class="s2">&quot;fs_desikan_roi&quot;</span><span class="p">,</span>
        <span class="n">target</span><span class="o">=</span><span class="s2">&quot;age&quot;</span><span class="p">,</span>
        <span class="n">split</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">,</span>
        <span class="n">streaming</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">transforms</span><span class="o">=</span><a href="../modules/generated/nidl.transforms.MultiViewsTransform.html#nidl.transforms.MultiViewsTransform" title="nidl.transforms.MultiViewsTransform" class="sphx-glr-backref-module-nidl-transforms sphx-glr-backref-type-py-class"><span class="n">MultiViewsTransform</span></a><span class="p">(</span>
            <a href="https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose" title="torchvision.transforms.Compose" class="sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span></a><span class="p">([</span><span class="n">sbm_transform</span><span class="p">,</span> <a href="https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose" title="torchvision.transforms.Compose" class="sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">contrast_transforms</span></a><span class="p">]),</span> <span class="n">n_views</span><span class="o">=</span><span class="mi">2</span>
        <span class="p">),</span>
    <span class="p">),</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
</pre></div>
</div>
<p>Finally, we create the dataloaders for evaluating the learned representations
on age prediction. We don’t apply any data augmentation here.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_vbm_train</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"><span class="n">DataLoader</span></a><span class="p">(</span>
    <a href="../modules/generated/nidl.datasets.OpenBHB.html#nidl.datasets.OpenBHB" title="nidl.datasets.OpenBHB" class="sphx-glr-backref-module-nidl-datasets sphx-glr-backref-type-py-class"><span class="n">OpenBHB</span></a><span class="p">(</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_dir</span></a><span class="p">,</span>
        <span class="n">modality</span><span class="o">=</span><span class="s2">&quot;vbm_roi&quot;</span><span class="p">,</span>
        <span class="n">target</span><span class="o">=</span><span class="s2">&quot;age&quot;</span><span class="p">,</span>
        <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
        <span class="n">transforms</span><span class="o">=</span><span class="n">vbm_transform</span><span class="p">,</span>
    <span class="p">),</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_vbm_test</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"><span class="n">DataLoader</span></a><span class="p">(</span>
    <a href="../modules/generated/nidl.datasets.OpenBHB.html#nidl.datasets.OpenBHB" title="nidl.datasets.OpenBHB" class="sphx-glr-backref-module-nidl-datasets sphx-glr-backref-type-py-class"><span class="n">OpenBHB</span></a><span class="p">(</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_dir</span></a><span class="p">,</span>
        <span class="n">modality</span><span class="o">=</span><span class="s2">&quot;vbm_roi&quot;</span><span class="p">,</span>
        <span class="n">target</span><span class="o">=</span><span class="s2">&quot;age&quot;</span><span class="p">,</span>
        <span class="n">split</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">,</span>
        <span class="n">transforms</span><span class="o">=</span><span class="n">vbm_transform</span><span class="p">,</span>
    <span class="p">),</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_sbm_train</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"><span class="n">DataLoader</span></a><span class="p">(</span>
    <a href="../modules/generated/nidl.datasets.OpenBHB.html#nidl.datasets.OpenBHB" title="nidl.datasets.OpenBHB" class="sphx-glr-backref-module-nidl-datasets sphx-glr-backref-type-py-class"><span class="n">OpenBHB</span></a><span class="p">(</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_dir</span></a><span class="p">,</span>
        <span class="n">modality</span><span class="o">=</span><span class="s2">&quot;fs_desikan_roi&quot;</span><span class="p">,</span>
        <span class="n">target</span><span class="o">=</span><span class="s2">&quot;age&quot;</span><span class="p">,</span>
        <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
        <span class="n">transforms</span><span class="o">=</span><span class="n">sbm_transform</span><span class="p">,</span>
    <span class="p">),</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_sbm_test</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"><span class="n">DataLoader</span></a><span class="p">(</span>
    <a href="../modules/generated/nidl.datasets.OpenBHB.html#nidl.datasets.OpenBHB" title="nidl.datasets.OpenBHB" class="sphx-glr-backref-module-nidl-datasets sphx-glr-backref-type-py-class"><span class="n">OpenBHB</span></a><span class="p">(</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#str" title="builtins.str" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_dir</span></a><span class="p">,</span>
        <span class="n">modality</span><span class="o">=</span><span class="s2">&quot;fs_desikan_roi&quot;</span><span class="p">,</span>
        <span class="n">target</span><span class="o">=</span><span class="s2">&quot;age&quot;</span><span class="p">,</span>
        <span class="n">split</span><span class="o">=</span><span class="s2">&quot;val&quot;</span><span class="p">,</span>
        <span class="n">transforms</span><span class="o">=</span><span class="n">sbm_transform</span><span class="p">,</span>
    <span class="p">),</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="p">,</span>
    <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">num_workers</span></a><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Small hack to avoid returning the target in the dataloaders since we aim</span>
<span class="c1"># at transforming these datasets without their targets.</span>
<a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_vbm_train</span></a><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="kc">None</span>
<a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_vbm_test</span></a><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="kc">None</span>
<a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_sbm_train</span></a><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="kc">None</span>
<a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_sbm_test</span></a><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
</pre></div>
</div>
</section>
<section id="training-of-y-aware-contrastive-learning-models">
<h2>Training of y-Aware Contrastive Learning models<a class="headerlink" href="#training-of-y-aware-contrastive-learning-models" title="Link to this heading">¶</a></h2>
<p>We can now instantiate and train two y-Aware Contrastive Learning models (one
for VBM and another for SBM). We use the age as weak label to impose similar
representations of samples with close age for both models.</p>
<p>Since we work with tabular data, we can use a simple MLP as encoder. For
VBM data, the input dimension is 284 and we compress the data to a 32-d
vector. SBM data is flattened to a 272-d vector (68 regions * 4 features)
and we also compress it to a 32-d vector.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.pytorch.org/vision/main/generated/torchvision.ops.MLP.html#torchvision.ops.MLP" title="torchvision.ops.MLP" class="sphx-glr-backref-module-torchvision-ops sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">vbm_encoder</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/vision/main/generated/torchvision.ops.MLP.html#torchvision.ops.MLP" title="torchvision.ops.MLP" class="sphx-glr-backref-module-torchvision-ops sphx-glr-backref-type-py-class"><span class="n">MLP</span></a><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">284</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">latent_size</span></a><span class="p">])</span>
<a href="https://docs.pytorch.org/vision/main/generated/torchvision.ops.MLP.html#torchvision.ops.MLP" title="torchvision.ops.MLP" class="sphx-glr-backref-module-torchvision-ops sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sbm_encoder</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/vision/main/generated/torchvision.ops.MLP.html#torchvision.ops.MLP" title="torchvision.ops.MLP" class="sphx-glr-backref-module-torchvision-ops sphx-glr-backref-type-py-class"><span class="n">MLP</span></a><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">272</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">latent_size</span></a><span class="p">])</span>
</pre></div>
</div>
<p>We limit the training to 10 epochs for the sake of time and we use a small
bandwidth for the Gaussian kernel in the y-Aware model compared to the
variance of the age in OpenBHB (sigma=4 vs std(age)=15).</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sigma</span></a> <span class="o">=</span> <span class="mi">4</span>
<a href="../modules/generated/nidl.estimators.ssl.YAwareContrastiveLearning.html#nidl.estimators.ssl.YAwareContrastiveLearning" title="nidl.estimators.ssl.YAwareContrastiveLearning" class="sphx-glr-backref-module-nidl-estimators-ssl sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">vbm_model</span></a> <span class="o">=</span> <a href="../modules/generated/nidl.estimators.ssl.YAwareContrastiveLearning.html#nidl.estimators.ssl.YAwareContrastiveLearning" title="nidl.estimators.ssl.YAwareContrastiveLearning" class="sphx-glr-backref-module-nidl-estimators-ssl sphx-glr-backref-type-py-class"><span class="n">YAwareContrastiveLearning</span></a><span class="p">(</span>
    <span class="n">encoder</span><span class="o">=</span><a href="https://docs.pytorch.org/vision/main/generated/torchvision.ops.MLP.html#torchvision.ops.MLP" title="torchvision.ops.MLP" class="sphx-glr-backref-module-torchvision-ops sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">vbm_encoder</span></a><span class="p">,</span>
    <span class="n">projection_head_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;input_dim&quot;</span><span class="p">:</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">latent_size</span></a><span class="p">,</span>
        <span class="s2">&quot;hidden_dim&quot;</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">latent_size</span></a><span class="p">,</span>
        <span class="s2">&quot;output_dim&quot;</span><span class="p">:</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">latent_size</span></a><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">bandwidth</span><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sigma</span></a><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">enable_checkpointing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<a href="../modules/generated/nidl.estimators.ssl.YAwareContrastiveLearning.html#nidl.estimators.ssl.YAwareContrastiveLearning" title="nidl.estimators.ssl.YAwareContrastiveLearning" class="sphx-glr-backref-module-nidl-estimators-ssl sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sbm_model</span></a> <span class="o">=</span> <a href="../modules/generated/nidl.estimators.ssl.YAwareContrastiveLearning.html#nidl.estimators.ssl.YAwareContrastiveLearning" title="nidl.estimators.ssl.YAwareContrastiveLearning" class="sphx-glr-backref-module-nidl-estimators-ssl sphx-glr-backref-type-py-class"><span class="n">YAwareContrastiveLearning</span></a><span class="p">(</span>
    <span class="n">encoder</span><span class="o">=</span><a href="https://docs.pytorch.org/vision/main/generated/torchvision.ops.MLP.html#torchvision.ops.MLP" title="torchvision.ops.MLP" class="sphx-glr-backref-module-torchvision-ops sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sbm_encoder</span></a><span class="p">,</span>
    <span class="n">projection_head_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;input_dim&quot;</span><span class="p">:</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">latent_size</span></a><span class="p">,</span>
        <span class="s2">&quot;hidden_dim&quot;</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">latent_size</span></a><span class="p">,</span>
        <span class="s2">&quot;output_dim&quot;</span><span class="p">:</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">latent_size</span></a><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">bandwidth</span><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">sigma</span></a><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">enable_checkpointing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We train both models on their respective dataloaders.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="../modules/generated/nidl.estimators.BaseEstimator.html#nidl.estimators.BaseEstimator.fit" title="nidl.estimators.BaseEstimator.fit" class="sphx-glr-backref-module-nidl-estimators sphx-glr-backref-type-py-method"><span class="n">vbm_model</span><span class="o">.</span><span class="n">fit</span></a><span class="p">(</span>
    <a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_ssl_vbm</span></a><span class="p">,</span>
    <a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_ssl_vbm_test</span></a><span class="p">,</span>
<span class="p">)</span>

<a href="../modules/generated/nidl.estimators.BaseEstimator.html#nidl.estimators.BaseEstimator.fit" title="nidl.estimators.BaseEstimator.fit" class="sphx-glr-backref-module-nidl-estimators sphx-glr-backref-type-py-method"><span class="n">sbm_model</span><span class="o">.</span><span class="n">fit</span></a><span class="p">(</span>
    <a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_ssl_sbm</span></a><span class="p">,</span>
    <a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_ssl_sbm_test</span></a><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]
Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]
Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00&lt;?, ?it/s]
Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00&lt;00:00, 43.35it/s]
Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00&lt;00:00, 52.35it/s]

/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.

Training: |          | 0/? [00:00&lt;?, ?it/s]
Training: |          | 0/? [00:00&lt;?, ?it/s]
Epoch 0:   0%|          | 0/26 [00:00&lt;?, ?it/s]
Epoch 0:   4%|▍         | 1/26 [00:00&lt;00:24,  1.02it/s]
Epoch 0:   4%|▍         | 1/26 [00:00&lt;00:24,  1.02it/s, v_num=8, loss/train=11.70]
Epoch 0:   8%|▊         | 2/26 [00:01&lt;00:12,  1.91it/s, v_num=8, loss/train=11.70]
Epoch 0:   8%|▊         | 2/26 [00:01&lt;00:12,  1.89it/s, v_num=8, loss/train=11.80]
Epoch 0:  12%|█▏        | 3/26 [00:01&lt;00:08,  2.73it/s, v_num=8, loss/train=11.80]
Epoch 0:  12%|█▏        | 3/26 [00:01&lt;00:08,  2.72it/s, v_num=8, loss/train=11.70]
Epoch 0:  15%|█▌        | 4/26 [00:01&lt;00:06,  3.38it/s, v_num=8, loss/train=11.70]
Epoch 0:  15%|█▌        | 4/26 [00:01&lt;00:06,  3.38it/s, v_num=8, loss/train=11.60]
Epoch 0:  19%|█▉        | 5/26 [00:01&lt;00:05,  3.81it/s, v_num=8, loss/train=11.60]
Epoch 0:  19%|█▉        | 5/26 [00:01&lt;00:05,  3.81it/s, v_num=8, loss/train=11.70]
Epoch 0:  23%|██▎       | 6/26 [00:01&lt;00:04,  4.33it/s, v_num=8, loss/train=11.70]
Epoch 0:  23%|██▎       | 6/26 [00:01&lt;00:04,  4.32it/s, v_num=8, loss/train=11.60]
Epoch 0:  27%|██▋       | 7/26 [00:01&lt;00:03,  4.91it/s, v_num=8, loss/train=11.60]
Epoch 0:  27%|██▋       | 7/26 [00:01&lt;00:03,  4.90it/s, v_num=8, loss/train=11.60]
Epoch 0:  31%|███       | 8/26 [00:01&lt;00:03,  5.56it/s, v_num=8, loss/train=11.60]
Epoch 0:  31%|███       | 8/26 [00:01&lt;00:03,  5.56it/s, v_num=8, loss/train=11.70]
Epoch 0:  35%|███▍      | 9/26 [00:01&lt;00:02,  6.21it/s, v_num=8, loss/train=11.70]
Epoch 0:  35%|███▍      | 9/26 [00:01&lt;00:02,  6.21it/s, v_num=8, loss/train=11.70]
Epoch 0:  38%|███▊      | 10/26 [00:01&lt;00:02,  6.85it/s, v_num=8, loss/train=11.70]
Epoch 0:  38%|███▊      | 10/26 [00:01&lt;00:02,  6.85it/s, v_num=8, loss/train=11.60]
Epoch 0:  42%|████▏     | 11/26 [00:01&lt;00:02,  7.48it/s, v_num=8, loss/train=11.60]
Epoch 0:  42%|████▏     | 11/26 [00:01&lt;00:02,  7.48it/s, v_num=8, loss/train=11.60]
Epoch 0:  46%|████▌     | 12/26 [00:01&lt;00:01,  8.08it/s, v_num=8, loss/train=11.60]
Epoch 0:  46%|████▌     | 12/26 [00:01&lt;00:01,  8.08it/s, v_num=8, loss/train=11.60]
Epoch 0:  50%|█████     | 13/26 [00:01&lt;00:01,  8.68it/s, v_num=8, loss/train=11.60]
Epoch 0:  50%|█████     | 13/26 [00:01&lt;00:01,  8.68it/s, v_num=8, loss/train=11.70]
Epoch 0:  54%|█████▍    | 14/26 [00:01&lt;00:01,  9.28it/s, v_num=8, loss/train=11.70]
Epoch 0:  54%|█████▍    | 14/26 [00:01&lt;00:01,  9.28it/s, v_num=8, loss/train=11.60]
Epoch 0:  58%|█████▊    | 15/26 [00:01&lt;00:01,  9.78it/s, v_num=8, loss/train=11.60]
Epoch 0:  58%|█████▊    | 15/26 [00:01&lt;00:01,  9.78it/s, v_num=8, loss/train=11.70]
Epoch 0:  62%|██████▏   | 16/26 [00:01&lt;00:00, 10.39it/s, v_num=8, loss/train=11.70]
Epoch 0:  62%|██████▏   | 16/26 [00:01&lt;00:00, 10.39it/s, v_num=8, loss/train=11.60]
Epoch 0:  65%|██████▌   | 17/26 [00:01&lt;00:00, 10.99it/s, v_num=8, loss/train=11.60]
Epoch 0:  65%|██████▌   | 17/26 [00:01&lt;00:00, 10.99it/s, v_num=8, loss/train=11.70]
Epoch 0:  69%|██████▉   | 18/26 [00:01&lt;00:00, 11.59it/s, v_num=8, loss/train=11.70]
Epoch 0:  69%|██████▉   | 18/26 [00:01&lt;00:00, 11.59it/s, v_num=8, loss/train=11.70]
Epoch 0:  73%|███████▎  | 19/26 [00:01&lt;00:00, 12.16it/s, v_num=8, loss/train=11.70]
Epoch 0:  73%|███████▎  | 19/26 [00:01&lt;00:00, 12.16it/s, v_num=8, loss/train=11.60]
Epoch 0:  77%|███████▋  | 20/26 [00:01&lt;00:00, 12.75it/s, v_num=8, loss/train=11.60]
Epoch 0:  77%|███████▋  | 20/26 [00:01&lt;00:00, 12.75it/s, v_num=8, loss/train=11.60]
Epoch 0:  81%|████████  | 21/26 [00:01&lt;00:00, 13.30it/s, v_num=8, loss/train=11.60]
Epoch 0:  81%|████████  | 21/26 [00:01&lt;00:00, 13.29it/s, v_num=8, loss/train=11.60]
Epoch 0:  85%|████████▍ | 22/26 [00:01&lt;00:00, 13.87it/s, v_num=8, loss/train=11.60]
Epoch 0:  85%|████████▍ | 22/26 [00:01&lt;00:00, 13.87it/s, v_num=8, loss/train=11.70]
Epoch 0:  88%|████████▊ | 23/26 [00:01&lt;00:00, 14.45it/s, v_num=8, loss/train=11.70]
Epoch 0:  88%|████████▊ | 23/26 [00:01&lt;00:00, 14.44it/s, v_num=8, loss/train=11.70]
Epoch 0:  92%|█████████▏| 24/26 [00:01&lt;00:00, 15.00it/s, v_num=8, loss/train=11.70]
Epoch 0:  92%|█████████▏| 24/26 [00:01&lt;00:00, 14.99it/s, v_num=8, loss/train=11.70]
Epoch 0:  96%|█████████▌| 25/26 [00:01&lt;00:00, 15.54it/s, v_num=8, loss/train=11.70]
Epoch 0:  96%|█████████▌| 25/26 [00:01&lt;00:00, 15.54it/s, v_num=8, loss/train=11.60]
Epoch 0: 100%|██████████| 26/26 [00:01&lt;00:00, 16.11it/s, v_num=8, loss/train=11.60]
Epoch 0: 100%|██████████| 26/26 [00:01&lt;00:00, 16.11it/s, v_num=8, loss/train=8.410]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 54.01it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 41.32it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 51.21it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 45.85it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 54.72it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 60.04it/s]


Epoch 0: 100%|██████████| 26/26 [00:02&lt;00:00, 10.84it/s, v_num=8, loss/train=8.410, loss/val=11.60]
Epoch 0: 100%|██████████| 26/26 [00:02&lt;00:00, 10.84it/s, v_num=8, loss/train=8.410, loss/val=11.60]
Epoch 0:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.410, loss/val=11.60]
Epoch 1:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.410, loss/val=11.60]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 1:   4%|▍         | 1/26 [00:01&lt;00:40,  0.62it/s, v_num=8, loss/train=8.410, loss/val=11.60]
Epoch 1:   4%|▍         | 1/26 [00:01&lt;00:40,  0.62it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:   8%|▊         | 2/26 [00:01&lt;00:20,  1.20it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:   8%|▊         | 2/26 [00:01&lt;00:20,  1.20it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  12%|█▏        | 3/26 [00:01&lt;00:13,  1.70it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  12%|█▏        | 3/26 [00:01&lt;00:13,  1.70it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.24it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.24it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.67it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.67it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.15it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.15it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.66it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.65it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  31%|███       | 8/26 [00:01&lt;00:04,  4.16it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  31%|███       | 8/26 [00:01&lt;00:04,  4.16it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.66it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.66it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.15it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.15it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  42%|████▏     | 11/26 [00:01&lt;00:02,  5.63it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  42%|████▏     | 11/26 [00:01&lt;00:02,  5.63it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  46%|████▌     | 12/26 [00:01&lt;00:02,  6.11it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  46%|████▌     | 12/26 [00:01&lt;00:02,  6.11it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  50%|█████     | 13/26 [00:01&lt;00:01,  6.59it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  50%|█████     | 13/26 [00:01&lt;00:01,  6.59it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  54%|█████▍    | 14/26 [00:01&lt;00:01,  7.07it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  54%|█████▍    | 14/26 [00:01&lt;00:01,  7.07it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  58%|█████▊    | 15/26 [00:01&lt;00:01,  7.51it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  58%|█████▊    | 15/26 [00:01&lt;00:01,  7.51it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.98it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.98it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.45it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.45it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.92it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.91it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.38it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.38it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.84it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.84it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  81%|████████  | 21/26 [00:02&lt;00:00, 10.28it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  81%|████████  | 21/26 [00:02&lt;00:00, 10.28it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.73it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.73it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  88%|████████▊ | 23/26 [00:02&lt;00:00, 11.18it/s, v_num=8, loss/train=11.60, loss/val=11.60]
Epoch 1:  88%|████████▊ | 23/26 [00:02&lt;00:00, 11.18it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.63it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.63it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  96%|█████████▌| 25/26 [00:02&lt;00:00, 12.04it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1:  96%|█████████▌| 25/26 [00:02&lt;00:00, 12.04it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1: 100%|██████████| 26/26 [00:02&lt;00:00, 12.49it/s, v_num=8, loss/train=11.50, loss/val=11.60]
Epoch 1: 100%|██████████| 26/26 [00:02&lt;00:00, 12.49it/s, v_num=8, loss/train=8.470, loss/val=11.60]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 24.13it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 16.41it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 23.53it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 27.75it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 33.47it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 37.92it/s]


Epoch 1: 100%|██████████| 26/26 [00:02&lt;00:00,  8.96it/s, v_num=8, loss/train=8.470, loss/val=11.50]
Epoch 1: 100%|██████████| 26/26 [00:02&lt;00:00,  8.96it/s, v_num=8, loss/train=8.470, loss/val=11.50]
Epoch 1:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.470, loss/val=11.50]
Epoch 2:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.470, loss/val=11.50]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 2:   4%|▍         | 1/26 [00:01&lt;00:37,  0.66it/s, v_num=8, loss/train=8.470, loss/val=11.50]
Epoch 2:   4%|▍         | 1/26 [00:01&lt;00:37,  0.66it/s, v_num=8, loss/train=11.60, loss/val=11.50]
Epoch 2:   8%|▊         | 2/26 [00:01&lt;00:18,  1.27it/s, v_num=8, loss/train=11.60, loss/val=11.50]
Epoch 2:   8%|▊         | 2/26 [00:01&lt;00:18,  1.27it/s, v_num=8, loss/train=11.60, loss/val=11.50]
Epoch 2:  12%|█▏        | 3/26 [00:01&lt;00:13,  1.75it/s, v_num=8, loss/train=11.60, loss/val=11.50]
Epoch 2:  12%|█▏        | 3/26 [00:01&lt;00:13,  1.75it/s, v_num=8, loss/train=11.60, loss/val=11.50]
Epoch 2:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.08it/s, v_num=8, loss/train=11.60, loss/val=11.50]
Epoch 2:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.08it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.59it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.59it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.09it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.09it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.59it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.59it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  31%|███       | 8/26 [00:01&lt;00:04,  4.08it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  31%|███       | 8/26 [00:01&lt;00:04,  4.08it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.57it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.57it/s, v_num=8, loss/train=11.40, loss/val=11.50]
Epoch 2:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.05it/s, v_num=8, loss/train=11.40, loss/val=11.50]
Epoch 2:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.04it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.50it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.50it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.97it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.97it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  50%|█████     | 13/26 [00:02&lt;00:02,  6.44it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  50%|█████     | 13/26 [00:02&lt;00:02,  6.44it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.85it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.85it/s, v_num=8, loss/train=11.40, loss/val=11.50]
Epoch 2:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.31it/s, v_num=8, loss/train=11.40, loss/val=11.50]
Epoch 2:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.31it/s, v_num=8, loss/train=11.60, loss/val=11.50]
Epoch 2:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.76it/s, v_num=8, loss/train=11.60, loss/val=11.50]
Epoch 2:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.76it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.21it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.21it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.64it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.64it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.08it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.08it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.52it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.52it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  81%|████████  | 21/26 [00:02&lt;00:00,  9.95it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  81%|████████  | 21/26 [00:02&lt;00:00,  9.95it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.37it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.37it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.80it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.80it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.21it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.21it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.62it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.62it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2: 100%|██████████| 26/26 [00:02&lt;00:00, 12.05it/s, v_num=8, loss/train=11.50, loss/val=11.50]
Epoch 2: 100%|██████████| 26/26 [00:02&lt;00:00, 12.05it/s, v_num=8, loss/train=8.350, loss/val=11.50]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 78.11it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 110.39it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 131.58it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 145.95it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 154.90it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 149.55it/s]


Epoch 2: 100%|██████████| 26/26 [00:02&lt;00:00,  8.72it/s, v_num=8, loss/train=8.350, loss/val=11.40]
Epoch 2: 100%|██████████| 26/26 [00:02&lt;00:00,  8.72it/s, v_num=8, loss/train=8.350, loss/val=11.40]
Epoch 2:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.350, loss/val=11.40]
Epoch 3:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.350, loss/val=11.40]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 3:   4%|▍         | 1/26 [00:01&lt;00:30,  0.82it/s, v_num=8, loss/train=8.350, loss/val=11.40]
Epoch 3:   4%|▍         | 1/26 [00:01&lt;00:30,  0.82it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:   8%|▊         | 2/26 [00:01&lt;00:18,  1.27it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:   8%|▊         | 2/26 [00:01&lt;00:18,  1.27it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  12%|█▏        | 3/26 [00:01&lt;00:12,  1.80it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  12%|█▏        | 3/26 [00:01&lt;00:12,  1.80it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.32it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.31it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.78it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.78it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.20it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.20it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.67it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.67it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  31%|███       | 8/26 [00:01&lt;00:04,  4.13it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  31%|███       | 8/26 [00:01&lt;00:04,  4.12it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.59it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.59it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.06it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.06it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  42%|████▏     | 11/26 [00:01&lt;00:02,  5.54it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  42%|████▏     | 11/26 [00:01&lt;00:02,  5.54it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  46%|████▌     | 12/26 [00:02&lt;00:02,  6.00it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  46%|████▌     | 12/26 [00:02&lt;00:02,  6.00it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  50%|█████     | 13/26 [00:02&lt;00:02,  6.41it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  50%|█████     | 13/26 [00:02&lt;00:02,  6.41it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.86it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.86it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.33it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.33it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.79it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.79it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.25it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.25it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.70it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.70it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.15it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.15it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.60it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.59it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  81%|████████  | 21/26 [00:02&lt;00:00, 10.04it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  81%|████████  | 21/26 [00:02&lt;00:00, 10.04it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.48it/s, v_num=8, loss/train=11.50, loss/val=11.40]
Epoch 3:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.47it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.89it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.89it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.32it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.32it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.76it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.76it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3: 100%|██████████| 26/26 [00:02&lt;00:00, 12.20it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 3: 100%|██████████| 26/26 [00:02&lt;00:00, 12.20it/s, v_num=8, loss/train=8.260, loss/val=11.40]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 29.16it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 40.91it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 46.59it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 40.01it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 47.70it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 53.06it/s]


Epoch 3: 100%|██████████| 26/26 [00:02&lt;00:00,  8.93it/s, v_num=8, loss/train=8.260, loss/val=11.40]
Epoch 3: 100%|██████████| 26/26 [00:02&lt;00:00,  8.93it/s, v_num=8, loss/train=8.260, loss/val=11.40]
Epoch 3:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.260, loss/val=11.40]
Epoch 4:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.260, loss/val=11.40]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 4:   4%|▍         | 1/26 [00:01&lt;00:42,  0.60it/s, v_num=8, loss/train=8.260, loss/val=11.40]
Epoch 4:   4%|▍         | 1/26 [00:01&lt;00:42,  0.59it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:   8%|▊         | 2/26 [00:01&lt;00:20,  1.15it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:   8%|▊         | 2/26 [00:01&lt;00:20,  1.15it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  12%|█▏        | 3/26 [00:01&lt;00:13,  1.70it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  12%|█▏        | 3/26 [00:01&lt;00:13,  1.70it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.23it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.22it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.74it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.74it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.24it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.23it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.65it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.65it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  31%|███       | 8/26 [00:01&lt;00:04,  4.14it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  31%|███       | 8/26 [00:01&lt;00:04,  4.14it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.63it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.63it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.10it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.10it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.48it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.48it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.94it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.94it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  50%|█████     | 13/26 [00:02&lt;00:02,  6.40it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  50%|█████     | 13/26 [00:02&lt;00:02,  6.40it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.86it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.85it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.32it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.32it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.76it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.76it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.18it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.18it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.64it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.63it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.09it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.08it/s, v_num=8, loss/train=11.30, loss/val=11.40]
Epoch 4:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.53it/s, v_num=8, loss/train=11.30, loss/val=11.40]
Epoch 4:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.53it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  81%|████████  | 21/26 [00:02&lt;00:00,  9.96it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  81%|████████  | 21/26 [00:02&lt;00:00,  9.96it/s, v_num=8, loss/train=11.30, loss/val=11.40]
Epoch 4:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.40it/s, v_num=8, loss/train=11.30, loss/val=11.40]
Epoch 4:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.40it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.83it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.83it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.26it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.26it/s, v_num=8, loss/train=11.30, loss/val=11.40]
Epoch 4:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.68it/s, v_num=8, loss/train=11.30, loss/val=11.40]
Epoch 4:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.68it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4: 100%|██████████| 26/26 [00:02&lt;00:00, 12.12it/s, v_num=8, loss/train=11.40, loss/val=11.40]
Epoch 4: 100%|██████████| 26/26 [00:02&lt;00:00, 12.11it/s, v_num=8, loss/train=8.100, loss/val=11.40]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 79.45it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 113.23it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 135.39it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 140.63it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 152.73it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 148.30it/s]


Epoch 4: 100%|██████████| 26/26 [00:02&lt;00:00,  8.83it/s, v_num=8, loss/train=8.100, loss/val=11.30]
Epoch 4: 100%|██████████| 26/26 [00:02&lt;00:00,  8.82it/s, v_num=8, loss/train=8.100, loss/val=11.30]
Epoch 4:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.100, loss/val=11.30]
Epoch 5:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.100, loss/val=11.30]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 5:   4%|▍         | 1/26 [00:01&lt;00:35,  0.71it/s, v_num=8, loss/train=8.100, loss/val=11.30]
Epoch 5:   4%|▍         | 1/26 [00:01&lt;00:35,  0.70it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:   8%|▊         | 2/26 [00:01&lt;00:22,  1.07it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:   8%|▊         | 2/26 [00:01&lt;00:22,  1.07it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.57it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.57it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.07it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.06it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.55it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.55it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  23%|██▎       | 6/26 [00:02&lt;00:06,  2.92it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  23%|██▎       | 6/26 [00:02&lt;00:06,  2.92it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  27%|██▋       | 7/26 [00:02&lt;00:05,  3.38it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  27%|██▋       | 7/26 [00:02&lt;00:05,  3.38it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  31%|███       | 8/26 [00:02&lt;00:04,  3.84it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  31%|███       | 8/26 [00:02&lt;00:04,  3.84it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  35%|███▍      | 9/26 [00:02&lt;00:03,  4.30it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  35%|███▍      | 9/26 [00:02&lt;00:03,  4.30it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.76it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.76it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.22it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.22it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.67it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.67it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  50%|█████     | 13/26 [00:02&lt;00:02,  6.10it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  50%|█████     | 13/26 [00:02&lt;00:02,  6.10it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.55it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.54it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  58%|█████▊    | 15/26 [00:02&lt;00:01,  6.99it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  58%|█████▊    | 15/26 [00:02&lt;00:01,  6.99it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.42it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.42it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  65%|██████▌   | 17/26 [00:02&lt;00:01,  7.86it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  65%|██████▌   | 17/26 [00:02&lt;00:01,  7.86it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.30it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.30it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  73%|███████▎  | 19/26 [00:02&lt;00:00,  8.73it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  73%|███████▎  | 19/26 [00:02&lt;00:00,  8.73it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.16it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.16it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  81%|████████  | 21/26 [00:02&lt;00:00,  9.58it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  81%|████████  | 21/26 [00:02&lt;00:00,  9.58it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.01it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.01it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.42it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.42it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  92%|█████████▏| 24/26 [00:02&lt;00:00, 10.83it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5:  92%|█████████▏| 24/26 [00:02&lt;00:00, 10.82it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.24it/s, v_num=8, loss/train=11.40, loss/val=11.30]
Epoch 5:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.24it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5: 100%|██████████| 26/26 [00:02&lt;00:00, 11.66it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 5: 100%|██████████| 26/26 [00:02&lt;00:00, 11.66it/s, v_num=8, loss/train=8.140, loss/val=11.30]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 65.62it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 88.36it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 112.01it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 116.48it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 129.15it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 130.95it/s]


Epoch 5: 100%|██████████| 26/26 [00:03&lt;00:00,  8.59it/s, v_num=8, loss/train=8.140, loss/val=11.30]
Epoch 5: 100%|██████████| 26/26 [00:03&lt;00:00,  8.59it/s, v_num=8, loss/train=8.140, loss/val=11.30]
Epoch 5:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.140, loss/val=11.30]
Epoch 6:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.140, loss/val=11.30]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 6:   4%|▍         | 1/26 [00:01&lt;00:36,  0.68it/s, v_num=8, loss/train=8.140, loss/val=11.30]
Epoch 6:   4%|▍         | 1/26 [00:01&lt;00:36,  0.68it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:   8%|▊         | 2/26 [00:01&lt;00:18,  1.30it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:   8%|▊         | 2/26 [00:01&lt;00:18,  1.30it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  12%|█▏        | 3/26 [00:01&lt;00:12,  1.87it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  12%|█▏        | 3/26 [00:01&lt;00:12,  1.86it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.35it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.35it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.86it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.86it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.31it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.30it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.74it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.73it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  31%|███       | 8/26 [00:01&lt;00:04,  4.05it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  31%|███       | 8/26 [00:01&lt;00:04,  4.05it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.51it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.51it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.95it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.95it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.40it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.40it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.87it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.87it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  50%|█████     | 13/26 [00:02&lt;00:02,  6.33it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  50%|█████     | 13/26 [00:02&lt;00:02,  6.33it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.77it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.77it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.23it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.23it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.69it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.69it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.14it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.13it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.56it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.56it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.01it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.01it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.45it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.45it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  81%|████████  | 21/26 [00:02&lt;00:00,  9.89it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  81%|████████  | 21/26 [00:02&lt;00:00,  9.88it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.31it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.31it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.73it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.73it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.14it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.14it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.56it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.55it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6: 100%|██████████| 26/26 [00:02&lt;00:00, 11.98it/s, v_num=8, loss/train=11.30, loss/val=11.30]
Epoch 6: 100%|██████████| 26/26 [00:02&lt;00:00, 11.98it/s, v_num=8, loss/train=8.210, loss/val=11.30]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 79.17it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 119.13it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 145.08it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 151.91it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 166.68it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 163.78it/s]


Epoch 6: 100%|██████████| 26/26 [00:02&lt;00:00,  8.73it/s, v_num=8, loss/train=8.210, loss/val=11.20]
Epoch 6: 100%|██████████| 26/26 [00:02&lt;00:00,  8.72it/s, v_num=8, loss/train=8.210, loss/val=11.20]
Epoch 6:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.210, loss/val=11.20]
Epoch 7:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.210, loss/val=11.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 7:   4%|▍         | 1/26 [00:01&lt;00:45,  0.55it/s, v_num=8, loss/train=8.210, loss/val=11.20]
Epoch 7:   4%|▍         | 1/26 [00:01&lt;00:45,  0.55it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:   8%|▊         | 2/26 [00:01&lt;00:22,  1.08it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:   8%|▊         | 2/26 [00:01&lt;00:22,  1.08it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.60it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.60it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.11it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.11it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.62it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.62it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.10it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.10it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.51it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.51it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  31%|███       | 8/26 [00:02&lt;00:04,  3.97it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  31%|███       | 8/26 [00:02&lt;00:04,  3.97it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  35%|███▍      | 9/26 [00:02&lt;00:03,  4.44it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  35%|███▍      | 9/26 [00:02&lt;00:03,  4.44it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.91it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.91it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.37it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.37it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.81it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.81it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  50%|█████     | 13/26 [00:02&lt;00:02,  6.27it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  50%|█████     | 13/26 [00:02&lt;00:02,  6.27it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.73it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.73it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.19it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.19it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.64it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.64it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.08it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.08it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.53it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.53it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  73%|███████▎  | 19/26 [00:02&lt;00:00,  8.97it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  73%|███████▎  | 19/26 [00:02&lt;00:00,  8.97it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.42it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.42it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  81%|████████  | 21/26 [00:02&lt;00:00,  9.86it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  81%|████████  | 21/26 [00:02&lt;00:00,  9.86it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.27it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.27it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.71it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.71it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.12it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.12it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.55it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.55it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7: 100%|██████████| 26/26 [00:02&lt;00:00, 11.98it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 7: 100%|██████████| 26/26 [00:02&lt;00:00, 11.98it/s, v_num=8, loss/train=8.160, loss/val=11.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 64.74it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 22.28it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 24.58it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 31.57it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 38.08it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 43.03it/s]


Epoch 7: 100%|██████████| 26/26 [00:02&lt;00:00,  8.78it/s, v_num=8, loss/train=8.160, loss/val=11.20]
Epoch 7: 100%|██████████| 26/26 [00:02&lt;00:00,  8.78it/s, v_num=8, loss/train=8.160, loss/val=11.20]
Epoch 7:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.160, loss/val=11.20]
Epoch 8:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.160, loss/val=11.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 8:   4%|▍         | 1/26 [00:01&lt;00:26,  0.93it/s, v_num=8, loss/train=8.160, loss/val=11.20]
Epoch 8:   4%|▍         | 1/26 [00:01&lt;00:27,  0.92it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:   8%|▊         | 2/26 [00:01&lt;00:20,  1.15it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:   8%|▊         | 2/26 [00:01&lt;00:21,  1.14it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.61it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.61it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.13it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.13it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.63it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.63it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.09it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.09it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.58it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.58it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  31%|███       | 8/26 [00:01&lt;00:04,  4.02it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  31%|███       | 8/26 [00:01&lt;00:04,  4.02it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  35%|███▍      | 9/26 [00:02&lt;00:03,  4.46it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  35%|███▍      | 9/26 [00:02&lt;00:03,  4.46it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.91it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.91it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.36it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.36it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.82it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.82it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  50%|█████     | 13/26 [00:02&lt;00:02,  6.23it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  50%|█████     | 13/26 [00:02&lt;00:02,  6.23it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.67it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.67it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.11it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.10it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.55it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.55it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  65%|██████▌   | 17/26 [00:02&lt;00:01,  7.99it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  65%|██████▌   | 17/26 [00:02&lt;00:01,  7.99it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.43it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.43it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  73%|███████▎  | 19/26 [00:02&lt;00:00,  8.85it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  73%|███████▎  | 19/26 [00:02&lt;00:00,  8.85it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.28it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.28it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  81%|████████  | 21/26 [00:02&lt;00:00,  9.71it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  81%|████████  | 21/26 [00:02&lt;00:00,  9.71it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.14it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.14it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.54it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 8:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.54it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  92%|█████████▏| 24/26 [00:02&lt;00:00, 10.94it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  92%|█████████▏| 24/26 [00:02&lt;00:00, 10.93it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.32it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.32it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8: 100%|██████████| 26/26 [00:02&lt;00:00, 11.73it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 8: 100%|██████████| 26/26 [00:02&lt;00:00, 11.73it/s, v_num=8, loss/train=8.130, loss/val=11.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 67.54it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 76.74it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 95.17it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 107.13it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 116.59it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 115.51it/s]


Epoch 8: 100%|██████████| 26/26 [00:03&lt;00:00,  8.63it/s, v_num=8, loss/train=8.130, loss/val=11.20]
Epoch 8: 100%|██████████| 26/26 [00:03&lt;00:00,  8.62it/s, v_num=8, loss/train=8.130, loss/val=11.20]
Epoch 8:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.130, loss/val=11.20]
Epoch 9:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=8, loss/train=8.130, loss/val=11.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 9:   4%|▍         | 1/26 [00:01&lt;00:40,  0.62it/s, v_num=8, loss/train=8.130, loss/val=11.20]
Epoch 9:   4%|▍         | 1/26 [00:01&lt;00:40,  0.61it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:   8%|▊         | 2/26 [00:01&lt;00:20,  1.19it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:   8%|▊         | 2/26 [00:01&lt;00:20,  1.19it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.63it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.63it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.12it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.12it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.58it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.58it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.06it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.06it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.53it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.53it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  31%|███       | 8/26 [00:01&lt;00:04,  4.01it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  31%|███       | 8/26 [00:01&lt;00:04,  4.01it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  35%|███▍      | 9/26 [00:02&lt;00:03,  4.48it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  35%|███▍      | 9/26 [00:02&lt;00:03,  4.48it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.95it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.95it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.41it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.41it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.87it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.86it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  50%|█████     | 13/26 [00:02&lt;00:02,  6.32it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  50%|█████     | 13/26 [00:02&lt;00:02,  6.32it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.77it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.76it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.20it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.20it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.66it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.66it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.10it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.10it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.55it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.55it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.00it/s, v_num=8, loss/train=11.30, loss/val=11.20]
Epoch 9:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.00it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.44it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.44it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  81%|████████  | 21/26 [00:02&lt;00:00,  9.84it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  81%|████████  | 21/26 [00:02&lt;00:00,  9.84it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.27it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.27it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.70it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.70it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.13it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.13it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.54it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.54it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9: 100%|██████████| 26/26 [00:02&lt;00:00, 11.97it/s, v_num=8, loss/train=11.20, loss/val=11.20]
Epoch 9: 100%|██████████| 26/26 [00:02&lt;00:00, 11.97it/s, v_num=8, loss/train=8.080, loss/val=11.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 55.72it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 63.35it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 23.88it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 30.64it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 36.92it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 41.67it/s]


Epoch 9: 100%|██████████| 26/26 [00:02&lt;00:00,  8.70it/s, v_num=8, loss/train=8.080, loss/val=11.20]
Epoch 9: 100%|██████████| 26/26 [00:02&lt;00:00,  8.69it/s, v_num=8, loss/train=8.080, loss/val=11.20]
Epoch 9: 100%|██████████| 26/26 [00:02&lt;00:00,  8.69it/s, v_num=8, loss/train=8.080, loss/val=11.20]

Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Sanity Checking: |          | 0/? [00:00&lt;?, ?it/s]
Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00&lt;?, ?it/s]
Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00&lt;00:00, 52.28it/s]
Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00&lt;00:00, 52.20it/s]

/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.

Training: |          | 0/? [00:00&lt;?, ?it/s]
Training: |          | 0/? [00:00&lt;?, ?it/s]
Epoch 0:   0%|          | 0/26 [00:00&lt;?, ?it/s]
Epoch 0:   4%|▍         | 1/26 [00:01&lt;00:28,  0.89it/s]
Epoch 0:   4%|▍         | 1/26 [00:01&lt;00:28,  0.88it/s, v_num=9, loss/train=12.90]
Epoch 0:   8%|▊         | 2/26 [00:01&lt;00:13,  1.72it/s, v_num=9, loss/train=12.90]
Epoch 0:   8%|▊         | 2/26 [00:01&lt;00:13,  1.72it/s, v_num=9, loss/train=12.90]
Epoch 0:  12%|█▏        | 3/26 [00:01&lt;00:09,  2.43it/s, v_num=9, loss/train=12.90]
Epoch 0:  12%|█▏        | 3/26 [00:01&lt;00:09,  2.43it/s, v_num=9, loss/train=13.00]
Epoch 0:  15%|█▌        | 4/26 [00:01&lt;00:06,  3.19it/s, v_num=9, loss/train=13.00]
Epoch 0:  15%|█▌        | 4/26 [00:01&lt;00:06,  3.18it/s, v_num=9, loss/train=13.00]
Epoch 0:  19%|█▉        | 5/26 [00:01&lt;00:05,  3.82it/s, v_num=9, loss/train=13.00]
Epoch 0:  19%|█▉        | 5/26 [00:01&lt;00:05,  3.81it/s, v_num=9, loss/train=12.90]
Epoch 0:  23%|██▎       | 6/26 [00:01&lt;00:04,  4.44it/s, v_num=9, loss/train=12.90]
Epoch 0:  23%|██▎       | 6/26 [00:01&lt;00:04,  4.44it/s, v_num=9, loss/train=12.90]
Epoch 0:  27%|██▋       | 7/26 [00:01&lt;00:03,  5.07it/s, v_num=9, loss/train=12.90]
Epoch 0:  27%|██▋       | 7/26 [00:01&lt;00:03,  5.07it/s, v_num=9, loss/train=12.80]
Epoch 0:  31%|███       | 8/26 [00:01&lt;00:03,  5.69it/s, v_num=9, loss/train=12.80]
Epoch 0:  31%|███       | 8/26 [00:01&lt;00:03,  5.69it/s, v_num=9, loss/train=12.80]
Epoch 0:  35%|███▍      | 9/26 [00:01&lt;00:02,  6.29it/s, v_num=9, loss/train=12.80]
Epoch 0:  35%|███▍      | 9/26 [00:01&lt;00:02,  6.29it/s, v_num=9, loss/train=12.80]
Epoch 0:  38%|███▊      | 10/26 [00:01&lt;00:02,  6.92it/s, v_num=9, loss/train=12.80]
Epoch 0:  38%|███▊      | 10/26 [00:01&lt;00:02,  6.92it/s, v_num=9, loss/train=12.90]
Epoch 0:  42%|████▏     | 11/26 [00:01&lt;00:02,  7.36it/s, v_num=9, loss/train=12.90]
Epoch 0:  42%|████▏     | 11/26 [00:01&lt;00:02,  7.36it/s, v_num=9, loss/train=12.80]
Epoch 0:  46%|████▌     | 12/26 [00:01&lt;00:01,  7.99it/s, v_num=9, loss/train=12.80]
Epoch 0:  46%|████▌     | 12/26 [00:01&lt;00:01,  7.99it/s, v_num=9, loss/train=12.90]
Epoch 0:  50%|█████     | 13/26 [00:01&lt;00:01,  8.61it/s, v_num=9, loss/train=12.90]
Epoch 0:  50%|█████     | 13/26 [00:01&lt;00:01,  8.60it/s, v_num=9, loss/train=12.60]
Epoch 0:  54%|█████▍    | 14/26 [00:01&lt;00:01,  9.22it/s, v_num=9, loss/train=12.60]
Epoch 0:  54%|█████▍    | 14/26 [00:01&lt;00:01,  9.21it/s, v_num=9, loss/train=12.80]
Epoch 0:  58%|█████▊    | 15/26 [00:01&lt;00:01,  9.82it/s, v_num=9, loss/train=12.80]
Epoch 0:  58%|█████▊    | 15/26 [00:01&lt;00:01,  9.82it/s, v_num=9, loss/train=12.60]
Epoch 0:  62%|██████▏   | 16/26 [00:01&lt;00:00, 10.43it/s, v_num=9, loss/train=12.60]
Epoch 0:  62%|██████▏   | 16/26 [00:01&lt;00:00, 10.43it/s, v_num=9, loss/train=12.70]
Epoch 0:  65%|██████▌   | 17/26 [00:01&lt;00:00, 11.03it/s, v_num=9, loss/train=12.70]
Epoch 0:  65%|██████▌   | 17/26 [00:01&lt;00:00, 11.03it/s, v_num=9, loss/train=12.60]
Epoch 0:  69%|██████▉   | 18/26 [00:01&lt;00:00, 11.63it/s, v_num=9, loss/train=12.60]
Epoch 0:  69%|██████▉   | 18/26 [00:01&lt;00:00, 11.63it/s, v_num=9, loss/train=12.60]
Epoch 0:  73%|███████▎  | 19/26 [00:01&lt;00:00, 12.22it/s, v_num=9, loss/train=12.60]
Epoch 0:  73%|███████▎  | 19/26 [00:01&lt;00:00, 12.22it/s, v_num=9, loss/train=12.80]
Epoch 0:  77%|███████▋  | 20/26 [00:01&lt;00:00, 12.81it/s, v_num=9, loss/train=12.80]
Epoch 0:  77%|███████▋  | 20/26 [00:01&lt;00:00, 12.80it/s, v_num=9, loss/train=12.70]
Epoch 0:  81%|████████  | 21/26 [00:01&lt;00:00, 13.31it/s, v_num=9, loss/train=12.70]
Epoch 0:  81%|████████  | 21/26 [00:01&lt;00:00, 13.31it/s, v_num=9, loss/train=12.80]
Epoch 0:  85%|████████▍ | 22/26 [00:01&lt;00:00, 13.88it/s, v_num=9, loss/train=12.80]
Epoch 0:  85%|████████▍ | 22/26 [00:01&lt;00:00, 13.88it/s, v_num=9, loss/train=12.80]
Epoch 0:  88%|████████▊ | 23/26 [00:01&lt;00:00, 14.45it/s, v_num=9, loss/train=12.80]
Epoch 0:  88%|████████▊ | 23/26 [00:01&lt;00:00, 14.45it/s, v_num=9, loss/train=12.70]
Epoch 0:  92%|█████████▏| 24/26 [00:01&lt;00:00, 15.01it/s, v_num=9, loss/train=12.70]
Epoch 0:  92%|█████████▏| 24/26 [00:01&lt;00:00, 15.01it/s, v_num=9, loss/train=12.60]
Epoch 0:  96%|█████████▌| 25/26 [00:01&lt;00:00, 15.57it/s, v_num=9, loss/train=12.60]
Epoch 0:  96%|█████████▌| 25/26 [00:01&lt;00:00, 15.57it/s, v_num=9, loss/train=12.60]
Epoch 0: 100%|██████████| 26/26 [00:01&lt;00:00, 16.14it/s, v_num=9, loss/train=12.60]
Epoch 0: 100%|██████████| 26/26 [00:01&lt;00:00, 16.13it/s, v_num=9, loss/train=9.700]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 58.91it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 76.50it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 87.12it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 102.99it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 111.72it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 115.48it/s]


Epoch 0: 100%|██████████| 26/26 [00:02&lt;00:00, 10.85it/s, v_num=9, loss/train=9.700, loss/val=12.60]
Epoch 0: 100%|██████████| 26/26 [00:02&lt;00:00, 10.85it/s, v_num=9, loss/train=9.700, loss/val=12.60]
Epoch 0:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=9.700, loss/val=12.60]
Epoch 1:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=9.700, loss/val=12.60]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 1:   4%|▍         | 1/26 [00:01&lt;00:39,  0.63it/s, v_num=9, loss/train=9.700, loss/val=12.60]
Epoch 1:   4%|▍         | 1/26 [00:01&lt;00:40,  0.62it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:   8%|▊         | 2/26 [00:01&lt;00:21,  1.13it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:   8%|▊         | 2/26 [00:01&lt;00:21,  1.13it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:  12%|█▏        | 3/26 [00:01&lt;00:13,  1.68it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:  12%|█▏        | 3/26 [00:01&lt;00:13,  1.68it/s, v_num=9, loss/train=12.70, loss/val=12.60]
Epoch 1:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.20it/s, v_num=9, loss/train=12.70, loss/val=12.60]
Epoch 1:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.20it/s, v_num=9, loss/train=12.70, loss/val=12.60]
Epoch 1:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.72it/s, v_num=9, loss/train=12.70, loss/val=12.60]
Epoch 1:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.71it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.22it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.22it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.71it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.71it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:  31%|███       | 8/26 [00:01&lt;00:04,  4.20it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:  31%|███       | 8/26 [00:01&lt;00:04,  4.20it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.69it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.68it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.16it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.15it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  42%|████▏     | 11/26 [00:01&lt;00:02,  5.63it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  42%|████▏     | 11/26 [00:01&lt;00:02,  5.63it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  46%|████▌     | 12/26 [00:01&lt;00:02,  6.05it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  46%|████▌     | 12/26 [00:01&lt;00:02,  6.05it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:  50%|█████     | 13/26 [00:01&lt;00:01,  6.51it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:  50%|█████     | 13/26 [00:01&lt;00:01,  6.50it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.97it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.97it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.45it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.44it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.91it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.91it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.37it/s, v_num=9, loss/train=12.60, loss/val=12.60]
Epoch 1:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.37it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.84it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.84it/s, v_num=9, loss/train=12.30, loss/val=12.60]
Epoch 1:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.30it/s, v_num=9, loss/train=12.30, loss/val=12.60]
Epoch 1:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.30it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.75it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.75it/s, v_num=9, loss/train=12.30, loss/val=12.60]
Epoch 1:  81%|████████  | 21/26 [00:02&lt;00:00, 10.20it/s, v_num=9, loss/train=12.30, loss/val=12.60]
Epoch 1:  81%|████████  | 21/26 [00:02&lt;00:00, 10.20it/s, v_num=9, loss/train=12.40, loss/val=12.60]
Epoch 1:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.63it/s, v_num=9, loss/train=12.40, loss/val=12.60]
Epoch 1:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.63it/s, v_num=9, loss/train=12.30, loss/val=12.60]
Epoch 1:  88%|████████▊ | 23/26 [00:02&lt;00:00, 11.08it/s, v_num=9, loss/train=12.30, loss/val=12.60]
Epoch 1:  88%|████████▊ | 23/26 [00:02&lt;00:00, 11.08it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.51it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.50it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.94it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.94it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1: 100%|██████████| 26/26 [00:02&lt;00:00, 12.38it/s, v_num=9, loss/train=12.50, loss/val=12.60]
Epoch 1: 100%|██████████| 26/26 [00:02&lt;00:00, 12.38it/s, v_num=9, loss/train=9.300, loss/val=12.60]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 68.86it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 99.88it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 123.38it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 142.34it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 148.85it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 148.31it/s]


Epoch 1: 100%|██████████| 26/26 [00:02&lt;00:00,  9.05it/s, v_num=9, loss/train=9.300, loss/val=12.40]
Epoch 1: 100%|██████████| 26/26 [00:02&lt;00:00,  9.04it/s, v_num=9, loss/train=9.300, loss/val=12.40]
Epoch 1:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=9.300, loss/val=12.40]
Epoch 2:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=9.300, loss/val=12.40]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 2:   4%|▍         | 1/26 [00:01&lt;00:43,  0.58it/s, v_num=9, loss/train=9.300, loss/val=12.40]
Epoch 2:   4%|▍         | 1/26 [00:01&lt;00:43,  0.58it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:   8%|▊         | 2/26 [00:01&lt;00:21,  1.10it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:   8%|▊         | 2/26 [00:01&lt;00:21,  1.10it/s, v_num=9, loss/train=12.50, loss/val=12.40]
Epoch 2:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.64it/s, v_num=9, loss/train=12.50, loss/val=12.40]
Epoch 2:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.64it/s, v_num=9, loss/train=12.50, loss/val=12.40]
Epoch 2:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.17it/s, v_num=9, loss/train=12.50, loss/val=12.40]
Epoch 2:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.16it/s, v_num=9, loss/train=12.50, loss/val=12.40]
Epoch 2:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.64it/s, v_num=9, loss/train=12.50, loss/val=12.40]
Epoch 2:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.64it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.12it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.12it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.59it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.59it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  31%|███       | 8/26 [00:01&lt;00:04,  4.07it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  31%|███       | 8/26 [00:01&lt;00:04,  4.07it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.55it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.54it/s, v_num=9, loss/train=12.50, loss/val=12.40]
Epoch 2:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.02it/s, v_num=9, loss/train=12.50, loss/val=12.40]
Epoch 2:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.02it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.50it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.50it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.96it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.96it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  50%|█████     | 13/26 [00:02&lt;00:02,  6.42it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  50%|█████     | 13/26 [00:02&lt;00:02,  6.42it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.89it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.89it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.35it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.35it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.81it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.81it/s, v_num=9, loss/train=12.50, loss/val=12.40]
Epoch 2:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.27it/s, v_num=9, loss/train=12.50, loss/val=12.40]
Epoch 2:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.27it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.71it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.71it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.17it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.16it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.62it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.62it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  81%|████████  | 21/26 [00:02&lt;00:00, 10.07it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  81%|████████  | 21/26 [00:02&lt;00:00, 10.07it/s, v_num=9, loss/train=12.20, loss/val=12.40]
Epoch 2:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.50it/s, v_num=9, loss/train=12.20, loss/val=12.40]
Epoch 2:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.50it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.94it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.93it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.35it/s, v_num=9, loss/train=12.30, loss/val=12.40]
Epoch 2:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.35it/s, v_num=9, loss/train=12.20, loss/val=12.40]
Epoch 2:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.79it/s, v_num=9, loss/train=12.20, loss/val=12.40]
Epoch 2:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.78it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2: 100%|██████████| 26/26 [00:02&lt;00:00, 12.22it/s, v_num=9, loss/train=12.40, loss/val=12.40]
Epoch 2: 100%|██████████| 26/26 [00:02&lt;00:00, 12.22it/s, v_num=9, loss/train=8.850, loss/val=12.40]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 61.48it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 16.22it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 19.24it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 25.00it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 30.50it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 35.13it/s]


Epoch 2: 100%|██████████| 26/26 [00:02&lt;00:00,  8.87it/s, v_num=9, loss/train=8.850, loss/val=12.20]
Epoch 2: 100%|██████████| 26/26 [00:02&lt;00:00,  8.87it/s, v_num=9, loss/train=8.850, loss/val=12.20]
Epoch 2:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.850, loss/val=12.20]
Epoch 3:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.850, loss/val=12.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 3:   4%|▍         | 1/26 [00:01&lt;00:39,  0.63it/s, v_num=9, loss/train=8.850, loss/val=12.20]
Epoch 3:   4%|▍         | 1/26 [00:01&lt;00:39,  0.63it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:   8%|▊         | 2/26 [00:01&lt;00:21,  1.13it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:   8%|▊         | 2/26 [00:01&lt;00:21,  1.13it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  12%|█▏        | 3/26 [00:01&lt;00:13,  1.66it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  12%|█▏        | 3/26 [00:01&lt;00:13,  1.66it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.17it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.17it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.65it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.64it/s, v_num=9, loss/train=12.30, loss/val=12.20]
Epoch 3:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.12it/s, v_num=9, loss/train=12.30, loss/val=12.20]
Epoch 3:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.12it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.59it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.59it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  31%|███       | 8/26 [00:01&lt;00:04,  4.06it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  31%|███       | 8/26 [00:01&lt;00:04,  4.06it/s, v_num=9, loss/train=12.10, loss/val=12.20]
Epoch 3:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.54it/s, v_num=9, loss/train=12.10, loss/val=12.20]
Epoch 3:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.54it/s, v_num=9, loss/train=12.30, loss/val=12.20]
Epoch 3:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.02it/s, v_num=9, loss/train=12.30, loss/val=12.20]
Epoch 3:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.01it/s, v_num=9, loss/train=12.10, loss/val=12.20]
Epoch 3:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.48it/s, v_num=9, loss/train=12.10, loss/val=12.20]
Epoch 3:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.47it/s, v_num=9, loss/train=12.10, loss/val=12.20]
Epoch 3:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.84it/s, v_num=9, loss/train=12.10, loss/val=12.20]
Epoch 3:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.84it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  50%|█████     | 13/26 [00:02&lt;00:02,  6.30it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  50%|█████     | 13/26 [00:02&lt;00:02,  6.30it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.76it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.76it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.21it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.21it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.66it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.66it/s, v_num=9, loss/train=12.00, loss/val=12.20]
Epoch 3:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.10it/s, v_num=9, loss/train=12.00, loss/val=12.20]
Epoch 3:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.10it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.55it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.54it/s, v_num=9, loss/train=12.10, loss/val=12.20]
Epoch 3:  73%|███████▎  | 19/26 [00:02&lt;00:00,  8.99it/s, v_num=9, loss/train=12.10, loss/val=12.20]
Epoch 3:  73%|███████▎  | 19/26 [00:02&lt;00:00,  8.99it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.43it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.43it/s, v_num=9, loss/train=12.00, loss/val=12.20]
Epoch 3:  81%|████████  | 21/26 [00:02&lt;00:00,  9.85it/s, v_num=9, loss/train=12.00, loss/val=12.20]
Epoch 3:  81%|████████  | 21/26 [00:02&lt;00:00,  9.85it/s, v_num=9, loss/train=12.00, loss/val=12.20]
Epoch 3:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.27it/s, v_num=9, loss/train=12.00, loss/val=12.20]
Epoch 3:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.27it/s, v_num=9, loss/train=12.00, loss/val=12.20]
Epoch 3:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.67it/s, v_num=9, loss/train=12.00, loss/val=12.20]
Epoch 3:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.67it/s, v_num=9, loss/train=12.10, loss/val=12.20]
Epoch 3:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.10it/s, v_num=9, loss/train=12.10, loss/val=12.20]
Epoch 3:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.10it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.51it/s, v_num=9, loss/train=12.20, loss/val=12.20]
Epoch 3:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.51it/s, v_num=9, loss/train=12.00, loss/val=12.20]
Epoch 3: 100%|██████████| 26/26 [00:02&lt;00:00, 11.94it/s, v_num=9, loss/train=12.00, loss/val=12.20]
Epoch 3: 100%|██████████| 26/26 [00:02&lt;00:00, 11.94it/s, v_num=9, loss/train=8.820, loss/val=12.20]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 57.36it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 76.73it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 96.71it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 109.06it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 123.01it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 126.10it/s]


Epoch 3: 100%|██████████| 26/26 [00:03&lt;00:00,  8.62it/s, v_num=9, loss/train=8.820, loss/val=12.00]
Epoch 3: 100%|██████████| 26/26 [00:03&lt;00:00,  8.62it/s, v_num=9, loss/train=8.820, loss/val=12.00]
Epoch 3:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.820, loss/val=12.00]
Epoch 4:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.820, loss/val=12.00]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 4:   4%|▍         | 1/26 [00:01&lt;00:40,  0.62it/s, v_num=9, loss/train=8.820, loss/val=12.00]
Epoch 4:   4%|▍         | 1/26 [00:01&lt;00:40,  0.62it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:   8%|▊         | 2/26 [00:01&lt;00:21,  1.14it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:   8%|▊         | 2/26 [00:01&lt;00:21,  1.14it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  12%|█▏        | 3/26 [00:01&lt;00:13,  1.64it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.64it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.14it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.14it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.61it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.61it/s, v_num=9, loss/train=12.20, loss/val=12.00]
Epoch 4:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.09it/s, v_num=9, loss/train=12.20, loss/val=12.00]
Epoch 4:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.09it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.56it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.56it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  31%|███       | 8/26 [00:01&lt;00:04,  4.03it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  31%|███       | 8/26 [00:01&lt;00:04,  4.03it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  35%|███▍      | 9/26 [00:02&lt;00:03,  4.49it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  35%|███▍      | 9/26 [00:02&lt;00:03,  4.49it/s, v_num=9, loss/train=11.90, loss/val=12.00]
Epoch 4:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.96it/s, v_num=9, loss/train=11.90, loss/val=12.00]
Epoch 4:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.96it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.42it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.42it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.89it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.89it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  50%|█████     | 13/26 [00:02&lt;00:02,  6.33it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  50%|█████     | 13/26 [00:02&lt;00:02,  6.33it/s, v_num=9, loss/train=11.90, loss/val=12.00]
Epoch 4:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.79it/s, v_num=9, loss/train=11.90, loss/val=12.00]
Epoch 4:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.79it/s, v_num=9, loss/train=11.90, loss/val=12.00]
Epoch 4:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.25it/s, v_num=9, loss/train=11.90, loss/val=12.00]
Epoch 4:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.25it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.70it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.70it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.16it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.15it/s, v_num=9, loss/train=11.90, loss/val=12.00]
Epoch 4:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.61it/s, v_num=9, loss/train=11.90, loss/val=12.00]
Epoch 4:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.61it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.04it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.04it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.48it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.48it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  81%|████████  | 21/26 [00:02&lt;00:00,  9.90it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  81%|████████  | 21/26 [00:02&lt;00:00,  9.89it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.33it/s, v_num=9, loss/train=12.10, loss/val=12.00]
Epoch 4:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.33it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.77it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.76it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.19it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.18it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.60it/s, v_num=9, loss/train=12.00, loss/val=12.00]
Epoch 4:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.60it/s, v_num=9, loss/train=11.90, loss/val=12.00]
Epoch 4: 100%|██████████| 26/26 [00:02&lt;00:00, 12.03it/s, v_num=9, loss/train=11.90, loss/val=12.00]
Epoch 4: 100%|██████████| 26/26 [00:02&lt;00:00, 12.03it/s, v_num=9, loss/train=8.880, loss/val=12.00]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 58.40it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 25.18it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 29.63it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 32.38it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 38.49it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 43.26it/s]


Epoch 4: 100%|██████████| 26/26 [00:02&lt;00:00,  8.69it/s, v_num=9, loss/train=8.880, loss/val=11.90]
Epoch 4: 100%|██████████| 26/26 [00:02&lt;00:00,  8.68it/s, v_num=9, loss/train=8.880, loss/val=11.90]
Epoch 4:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.880, loss/val=11.90]
Epoch 5:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.880, loss/val=11.90]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 5:   4%|▍         | 1/26 [00:01&lt;00:37,  0.66it/s, v_num=9, loss/train=8.880, loss/val=11.90]
Epoch 5:   4%|▍         | 1/26 [00:01&lt;00:37,  0.66it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:   8%|▊         | 2/26 [00:01&lt;00:21,  1.10it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:   8%|▊         | 2/26 [00:01&lt;00:21,  1.10it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.62it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.62it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.14it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.13it/s, v_num=9, loss/train=12.00, loss/val=11.90]
Epoch 5:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.62it/s, v_num=9, loss/train=12.00, loss/val=11.90]
Epoch 5:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.62it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.11it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.11it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.58it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.58it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  31%|███       | 8/26 [00:01&lt;00:04,  4.05it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  31%|███       | 8/26 [00:01&lt;00:04,  4.05it/s, v_num=9, loss/train=12.10, loss/val=11.90]
Epoch 5:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.51it/s, v_num=9, loss/train=12.10, loss/val=11.90]
Epoch 5:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.51it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.96it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.96it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.40it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.39it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.75it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.75it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  50%|█████     | 13/26 [00:02&lt;00:02,  6.20it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  50%|█████     | 13/26 [00:02&lt;00:02,  6.19it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.64it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.64it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.09it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.09it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.54it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.54it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  65%|██████▌   | 17/26 [00:02&lt;00:01,  7.98it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  65%|██████▌   | 17/26 [00:02&lt;00:01,  7.98it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.42it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.41it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  73%|███████▎  | 19/26 [00:02&lt;00:00,  8.85it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  73%|███████▎  | 19/26 [00:02&lt;00:00,  8.85it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.28it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.28it/s, v_num=9, loss/train=12.00, loss/val=11.90]
Epoch 5:  81%|████████  | 21/26 [00:02&lt;00:00,  9.69it/s, v_num=9, loss/train=12.00, loss/val=11.90]
Epoch 5:  81%|████████  | 21/26 [00:02&lt;00:00,  9.69it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.09it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.08it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.51it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.51it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  92%|█████████▏| 24/26 [00:02&lt;00:00, 10.93it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5:  92%|█████████▏| 24/26 [00:02&lt;00:00, 10.92it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.34it/s, v_num=9, loss/train=11.80, loss/val=11.90]
Epoch 5:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.34it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5: 100%|██████████| 26/26 [00:02&lt;00:00, 11.76it/s, v_num=9, loss/train=11.90, loss/val=11.90]
Epoch 5: 100%|██████████| 26/26 [00:02&lt;00:00, 11.76it/s, v_num=9, loss/train=8.460, loss/val=11.90]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 28.57it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 31.32it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 43.71it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 54.62it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 63.01it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 64.04it/s]


Epoch 5: 100%|██████████| 26/26 [00:03&lt;00:00,  8.59it/s, v_num=9, loss/train=8.460, loss/val=11.80]
Epoch 5: 100%|██████████| 26/26 [00:03&lt;00:00,  8.58it/s, v_num=9, loss/train=8.460, loss/val=11.80]
Epoch 5:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.460, loss/val=11.80]
Epoch 6:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.460, loss/val=11.80]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 6:   4%|▍         | 1/26 [00:01&lt;00:40,  0.62it/s, v_num=9, loss/train=8.460, loss/val=11.80]
Epoch 6:   4%|▍         | 1/26 [00:01&lt;00:40,  0.62it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:   8%|▊         | 2/26 [00:01&lt;00:22,  1.07it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:   8%|▊         | 2/26 [00:01&lt;00:22,  1.06it/s, v_num=9, loss/train=11.70, loss/val=11.80]
Epoch 6:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.57it/s, v_num=9, loss/train=11.70, loss/val=11.80]
Epoch 6:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.57it/s, v_num=9, loss/train=11.90, loss/val=11.80]
Epoch 6:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.06it/s, v_num=9, loss/train=11.90, loss/val=11.80]
Epoch 6:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.06it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.55it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.55it/s, v_num=9, loss/train=11.70, loss/val=11.80]
Epoch 6:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.01it/s, v_num=9, loss/train=11.70, loss/val=11.80]
Epoch 6:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.01it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  27%|██▋       | 7/26 [00:02&lt;00:05,  3.47it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  27%|██▋       | 7/26 [00:02&lt;00:05,  3.47it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  31%|███       | 8/26 [00:02&lt;00:04,  3.93it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  31%|███       | 8/26 [00:02&lt;00:04,  3.93it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  35%|███▍      | 9/26 [00:02&lt;00:03,  4.40it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  35%|███▍      | 9/26 [00:02&lt;00:03,  4.40it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.87it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  38%|███▊      | 10/26 [00:02&lt;00:03,  4.86it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.29it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.29it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.72it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.71it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  50%|█████     | 13/26 [00:02&lt;00:02,  6.14it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  50%|█████     | 13/26 [00:02&lt;00:02,  6.14it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.55it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.55it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  58%|█████▊    | 15/26 [00:02&lt;00:01,  6.98it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  58%|█████▊    | 15/26 [00:02&lt;00:01,  6.98it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.40it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.40it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  65%|██████▌   | 17/26 [00:02&lt;00:01,  7.84it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  65%|██████▌   | 17/26 [00:02&lt;00:01,  7.83it/s, v_num=9, loss/train=11.90, loss/val=11.80]
Epoch 6:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.27it/s, v_num=9, loss/train=11.90, loss/val=11.80]
Epoch 6:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.27it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  73%|███████▎  | 19/26 [00:02&lt;00:00,  8.70it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  73%|███████▎  | 19/26 [00:02&lt;00:00,  8.70it/s, v_num=9, loss/train=11.70, loss/val=11.80]
Epoch 6:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.12it/s, v_num=9, loss/train=11.70, loss/val=11.80]
Epoch 6:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.12it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  81%|████████  | 21/26 [00:02&lt;00:00,  9.53it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  81%|████████  | 21/26 [00:02&lt;00:00,  9.53it/s, v_num=9, loss/train=11.70, loss/val=11.80]
Epoch 6:  85%|████████▍ | 22/26 [00:02&lt;00:00,  9.94it/s, v_num=9, loss/train=11.70, loss/val=11.80]
Epoch 6:  85%|████████▍ | 22/26 [00:02&lt;00:00,  9.94it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.33it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.33it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  92%|█████████▏| 24/26 [00:02&lt;00:00, 10.72it/s, v_num=9, loss/train=11.80, loss/val=11.80]
Epoch 6:  92%|█████████▏| 24/26 [00:02&lt;00:00, 10.72it/s, v_num=9, loss/train=11.70, loss/val=11.80]
Epoch 6:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.12it/s, v_num=9, loss/train=11.70, loss/val=11.80]
Epoch 6:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.11it/s, v_num=9, loss/train=11.70, loss/val=11.80]
Epoch 6: 100%|██████████| 26/26 [00:02&lt;00:00, 11.52it/s, v_num=9, loss/train=11.70, loss/val=11.80]
Epoch 6: 100%|██████████| 26/26 [00:02&lt;00:00, 11.52it/s, v_num=9, loss/train=8.570, loss/val=11.80]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 44.24it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 19.30it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 25.97it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 30.71it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 37.20it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 42.35it/s]


Epoch 6: 100%|██████████| 26/26 [00:03&lt;00:00,  8.43it/s, v_num=9, loss/train=8.570, loss/val=11.70]
Epoch 6: 100%|██████████| 26/26 [00:03&lt;00:00,  8.43it/s, v_num=9, loss/train=8.570, loss/val=11.70]
Epoch 6:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.570, loss/val=11.70]
Epoch 7:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.570, loss/val=11.70]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 7:   4%|▍         | 1/26 [00:01&lt;00:36,  0.69it/s, v_num=9, loss/train=8.570, loss/val=11.70]
Epoch 7:   4%|▍         | 1/26 [00:01&lt;00:36,  0.69it/s, v_num=9, loss/train=11.80, loss/val=11.70]
Epoch 7:   8%|▊         | 2/26 [00:01&lt;00:17,  1.34it/s, v_num=9, loss/train=11.80, loss/val=11.70]
Epoch 7:   8%|▊         | 2/26 [00:01&lt;00:18,  1.33it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  12%|█▏        | 3/26 [00:01&lt;00:12,  1.79it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  12%|█▏        | 3/26 [00:01&lt;00:12,  1.79it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.27it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.27it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.74it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.74it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.14it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.14it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.63it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.62it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  31%|███       | 8/26 [00:01&lt;00:04,  4.11it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  31%|███       | 8/26 [00:01&lt;00:04,  4.10it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.58it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.58it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.02it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.02it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.49it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.49it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.95it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.95it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  50%|█████     | 13/26 [00:02&lt;00:02,  6.35it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  50%|█████     | 13/26 [00:02&lt;00:02,  6.35it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.80it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.80it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.26it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.26it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.71it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.71it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.17it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.17it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.62it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.62it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.06it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.06it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.50it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.50it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  81%|████████  | 21/26 [00:02&lt;00:00,  9.94it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  81%|████████  | 21/26 [00:02&lt;00:00,  9.94it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.38it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.38it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.79it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.79it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.22it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.22it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.65it/s, v_num=9, loss/train=11.60, loss/val=11.70]
Epoch 7:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.65it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7: 100%|██████████| 26/26 [00:02&lt;00:00, 12.08it/s, v_num=9, loss/train=11.70, loss/val=11.70]
Epoch 7: 100%|██████████| 26/26 [00:02&lt;00:00, 12.08it/s, v_num=9, loss/train=8.550, loss/val=11.70]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 22.70it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 22.90it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 31.44it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 39.99it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 46.61it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 52.31it/s]


Epoch 7: 100%|██████████| 26/26 [00:02&lt;00:00,  8.82it/s, v_num=9, loss/train=8.550, loss/val=11.60]
Epoch 7: 100%|██████████| 26/26 [00:02&lt;00:00,  8.82it/s, v_num=9, loss/train=8.550, loss/val=11.60]
Epoch 7:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.550, loss/val=11.60]
Epoch 8:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.550, loss/val=11.60]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 8:   4%|▍         | 1/26 [00:01&lt;00:41,  0.60it/s, v_num=9, loss/train=8.550, loss/val=11.60]
Epoch 8:   4%|▍         | 1/26 [00:01&lt;00:41,  0.60it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:   8%|▊         | 2/26 [00:01&lt;00:20,  1.18it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:   8%|▊         | 2/26 [00:01&lt;00:20,  1.18it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  12%|█▏        | 3/26 [00:01&lt;00:13,  1.74it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  12%|█▏        | 3/26 [00:01&lt;00:13,  1.74it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.26it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  15%|█▌        | 4/26 [00:01&lt;00:09,  2.26it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.73it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  19%|█▉        | 5/26 [00:01&lt;00:07,  2.72it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.21it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.20it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.68it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.68it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  31%|███       | 8/26 [00:01&lt;00:04,  4.06it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  31%|███       | 8/26 [00:01&lt;00:04,  4.06it/s, v_num=9, loss/train=11.70, loss/val=11.60]
Epoch 8:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.53it/s, v_num=9, loss/train=11.70, loss/val=11.60]
Epoch 8:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.53it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.01it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.01it/s, v_num=9, loss/train=11.50, loss/val=11.60]
Epoch 8:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.43it/s, v_num=9, loss/train=11.50, loss/val=11.60]
Epoch 8:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.43it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.90it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.90it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  50%|█████     | 13/26 [00:02&lt;00:02,  6.37it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  50%|█████     | 13/26 [00:02&lt;00:02,  6.37it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.83it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.83it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.29it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.29it/s, v_num=9, loss/train=11.50, loss/val=11.60]
Epoch 8:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.75it/s, v_num=9, loss/train=11.50, loss/val=11.60]
Epoch 8:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.75it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.21it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.21it/s, v_num=9, loss/train=11.50, loss/val=11.60]
Epoch 8:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.66it/s, v_num=9, loss/train=11.50, loss/val=11.60]
Epoch 8:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.66it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.11it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.10it/s, v_num=9, loss/train=11.70, loss/val=11.60]
Epoch 8:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.55it/s, v_num=9, loss/train=11.70, loss/val=11.60]
Epoch 8:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.55it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  81%|████████  | 21/26 [00:02&lt;00:00,  9.95it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  81%|████████  | 21/26 [00:02&lt;00:00,  9.95it/s, v_num=9, loss/train=11.50, loss/val=11.60]
Epoch 8:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.39it/s, v_num=9, loss/train=11.50, loss/val=11.60]
Epoch 8:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.39it/s, v_num=9, loss/train=11.50, loss/val=11.60]
Epoch 8:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.82it/s, v_num=9, loss/train=11.50, loss/val=11.60]
Epoch 8:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.82it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.25it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.25it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.68it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.68it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8: 100%|██████████| 26/26 [00:02&lt;00:00, 12.12it/s, v_num=9, loss/train=11.60, loss/val=11.60]
Epoch 8: 100%|██████████| 26/26 [00:02&lt;00:00, 12.11it/s, v_num=9, loss/train=8.570, loss/val=11.60]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 85.91it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 125.08it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 149.93it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 166.84it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 179.36it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 172.99it/s]


Epoch 8: 100%|██████████| 26/26 [00:02&lt;00:00,  8.73it/s, v_num=9, loss/train=8.570, loss/val=11.50]
Epoch 8: 100%|██████████| 26/26 [00:02&lt;00:00,  8.73it/s, v_num=9, loss/train=8.570, loss/val=11.50]
Epoch 8:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.570, loss/val=11.50]
Epoch 9:   0%|          | 0/26 [00:00&lt;?, ?it/s, v_num=9, loss/train=8.570, loss/val=11.50]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Epoch 9:   4%|▍         | 1/26 [00:01&lt;00:32,  0.77it/s, v_num=9, loss/train=8.570, loss/val=11.50]
Epoch 9:   4%|▍         | 1/26 [00:01&lt;00:32,  0.77it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:   8%|▊         | 2/26 [00:01&lt;00:21,  1.09it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:   8%|▊         | 2/26 [00:01&lt;00:21,  1.09it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.63it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  12%|█▏        | 3/26 [00:01&lt;00:14,  1.63it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.14it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  15%|█▌        | 4/26 [00:01&lt;00:10,  2.14it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.58it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  19%|█▉        | 5/26 [00:01&lt;00:08,  2.58it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.07it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  23%|██▎       | 6/26 [00:01&lt;00:06,  3.07it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.57it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  27%|██▋       | 7/26 [00:01&lt;00:05,  3.57it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  31%|███       | 8/26 [00:01&lt;00:04,  4.06it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  31%|███       | 8/26 [00:01&lt;00:04,  4.06it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.54it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  35%|███▍      | 9/26 [00:01&lt;00:03,  4.54it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.02it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  38%|███▊      | 10/26 [00:01&lt;00:03,  5.02it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  42%|████▏     | 11/26 [00:01&lt;00:02,  5.50it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  42%|████▏     | 11/26 [00:02&lt;00:02,  5.50it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.94it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  46%|████▌     | 12/26 [00:02&lt;00:02,  5.94it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  50%|█████     | 13/26 [00:02&lt;00:02,  6.40it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  50%|█████     | 13/26 [00:02&lt;00:02,  6.40it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.86it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  54%|█████▍    | 14/26 [00:02&lt;00:01,  6.86it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.32it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  58%|█████▊    | 15/26 [00:02&lt;00:01,  7.32it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.76it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  62%|██████▏   | 16/26 [00:02&lt;00:01,  7.76it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.18it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  65%|██████▌   | 17/26 [00:02&lt;00:01,  8.18it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.62it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  69%|██████▉   | 18/26 [00:02&lt;00:00,  8.61it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.06it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  73%|███████▎  | 19/26 [00:02&lt;00:00,  9.05it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.49it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  77%|███████▋  | 20/26 [00:02&lt;00:00,  9.49it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  81%|████████  | 21/26 [00:02&lt;00:00,  9.93it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  81%|████████  | 21/26 [00:02&lt;00:00,  9.93it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.35it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  85%|████████▍ | 22/26 [00:02&lt;00:00, 10.35it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.78it/s, v_num=9, loss/train=11.60, loss/val=11.50]
Epoch 9:  88%|████████▊ | 23/26 [00:02&lt;00:00, 10.77it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.18it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  92%|█████████▏| 24/26 [00:02&lt;00:00, 11.18it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.59it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9:  96%|█████████▌| 25/26 [00:02&lt;00:00, 11.59it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9: 100%|██████████| 26/26 [00:02&lt;00:00, 12.02it/s, v_num=9, loss/train=11.50, loss/val=11.50]
Epoch 9: 100%|██████████| 26/26 [00:02&lt;00:00, 12.02it/s, v_num=9, loss/train=8.460, loss/val=11.50]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(


Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation: |          | 0/? [00:00&lt;?, ?it/s]

Validation DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]

Validation DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 12.02it/s]

Validation DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 21.60it/s]

Validation DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 30.15it/s]

Validation DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 35.65it/s]

Validation DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 34.88it/s]

Validation DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 39.17it/s]


Epoch 9: 100%|██████████| 26/26 [00:02&lt;00:00,  8.68it/s, v_num=9, loss/train=8.460, loss/val=11.50]
Epoch 9: 100%|██████████| 26/26 [00:02&lt;00:00,  8.68it/s, v_num=9, loss/train=8.460, loss/val=11.50]
Epoch 9: 100%|██████████| 26/26 [00:02&lt;00:00,  8.68it/s, v_num=9, loss/train=8.460, loss/val=11.50]

YAwareContrastiveLearning(
  (encoder): MLP(
    (0): Linear(in_features=272, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=64, out_features=32, bias=True)
    (4): Dropout(p=0.0, inplace=False)
  )
  (projection_head): YAwareProjectionHead(
    (layers): Sequential(
      (0): Linear(in_features=32, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=32, bias=True)
    )
  )
  (loss): YAwareInfoNCE(
    (sim_metric): PairwiseCosineSimilarity()
  )
)
</pre></div>
</div>
</section>
<section id="visualization-and-evaluation-of-the-learned-representations">
<h2>Visualization and evaluation of the learned representations<a class="headerlink" href="#visualization-and-evaluation-of-the-learned-representations" title="Link to this heading">¶</a></h2>
<p>In order to visualize the learned representations of both models, we apply
a widely used dimensionality reduction technique: Multi-Dimensional Scaling
(MDS). This technique project the points in a lower-dimensional space such
that the pairwise distances between points are preserved as much as possible.
Then, we evaluate the learned representations on age prediction using linear
regression and KNN regression.</p>
<p>We first extract the embeddings of the training and test sets for both VBM
and SBM data.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Z_train_vbm</span></a> <span class="o">=</span> <a href="../modules/generated/nidl.estimators.BaseEstimator.html#nidl.estimators.BaseEstimator.transform" title="nidl.estimators.BaseEstimator.transform" class="sphx-glr-backref-module-nidl-estimators sphx-glr-backref-type-py-method"><span class="n">vbm_model</span><span class="o">.</span><span class="n">transform</span></a><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_vbm_train</span></a><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<a href="https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Z_test_vbm</span></a> <span class="o">=</span> <a href="../modules/generated/nidl.estimators.BaseEstimator.html#nidl.estimators.BaseEstimator.transform" title="nidl.estimators.BaseEstimator.transform" class="sphx-glr-backref-module-nidl-estimators sphx-glr-backref-type-py-method"><span class="n">vbm_model</span><span class="o">.</span><span class="n">transform</span></a><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_vbm_test</span></a><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<a href="https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Z_train_sbm</span></a> <span class="o">=</span> <a href="../modules/generated/nidl.estimators.BaseEstimator.html#nidl.estimators.BaseEstimator.transform" title="nidl.estimators.BaseEstimator.transform" class="sphx-glr-backref-module-nidl-estimators sphx-glr-backref-type-py-method"><span class="n">sbm_model</span><span class="o">.</span><span class="n">transform</span></a><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_sbm_train</span></a><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<a href="https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Z_test_sbm</span></a> <span class="o">=</span> <a href="../modules/generated/nidl.estimators.BaseEstimator.html#nidl.estimators.BaseEstimator.transform" title="nidl.estimators.BaseEstimator.transform" class="sphx-glr-backref-module-nidl-estimators sphx-glr-backref-type-py-method"><span class="n">sbm_model</span><span class="o">.</span><span class="n">transform</span></a><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/data.html#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader" class="sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_sbm_test</span></a><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Predicting: |          | 0/? [00:00&lt;?, ?it/s]
Predicting: |          | 0/? [00:00&lt;?, ?it/s]
Predicting DataLoader 0:   0%|          | 0/26 [00:00&lt;?, ?it/s]
Predicting DataLoader 0:   4%|▍         | 1/26 [00:00&lt;00:00, 55.57it/s]
Predicting DataLoader 0:   8%|▊         | 2/26 [00:00&lt;00:00, 73.41it/s]
Predicting DataLoader 0:  12%|█▏        | 3/26 [00:00&lt;00:00, 102.15it/s]
Predicting DataLoader 0:  15%|█▌        | 4/26 [00:00&lt;00:00, 106.26it/s]
Predicting DataLoader 0:  19%|█▉        | 5/26 [00:00&lt;00:00, 52.64it/s]
Predicting DataLoader 0:  23%|██▎       | 6/26 [00:00&lt;00:00, 24.18it/s]
Predicting DataLoader 0:  27%|██▋       | 7/26 [00:00&lt;00:00, 27.21it/s]
Predicting DataLoader 0:  31%|███       | 8/26 [00:00&lt;00:00, 30.94it/s]
Predicting DataLoader 0:  35%|███▍      | 9/26 [00:00&lt;00:00, 32.03it/s]
Predicting DataLoader 0:  38%|███▊      | 10/26 [00:00&lt;00:00, 32.79it/s]
Predicting DataLoader 0:  42%|████▏     | 11/26 [00:00&lt;00:00, 34.63it/s]
Predicting DataLoader 0:  46%|████▌     | 12/26 [00:00&lt;00:00, 37.04it/s]
Predicting DataLoader 0:  50%|█████     | 13/26 [00:00&lt;00:00, 39.50it/s]
Predicting DataLoader 0:  54%|█████▍    | 14/26 [00:00&lt;00:00, 41.92it/s]
Predicting DataLoader 0:  58%|█████▊    | 15/26 [00:00&lt;00:00, 42.72it/s]
Predicting DataLoader 0:  62%|██████▏   | 16/26 [00:00&lt;00:00, 36.95it/s]
Predicting DataLoader 0:  65%|██████▌   | 17/26 [00:00&lt;00:00, 39.08it/s]
Predicting DataLoader 0:  69%|██████▉   | 18/26 [00:00&lt;00:00, 41.21it/s]
Predicting DataLoader 0:  73%|███████▎  | 19/26 [00:00&lt;00:00, 43.33it/s]
Predicting DataLoader 0:  77%|███████▋  | 20/26 [00:00&lt;00:00, 45.43it/s]
Predicting DataLoader 0:  81%|████████  | 21/26 [00:00&lt;00:00, 47.54it/s]
Predicting DataLoader 0:  85%|████████▍ | 22/26 [00:00&lt;00:00, 49.67it/s]
Predicting DataLoader 0:  88%|████████▊ | 23/26 [00:00&lt;00:00, 51.83it/s]
Predicting DataLoader 0:  92%|█████████▏| 24/26 [00:00&lt;00:00, 53.81it/s]
Predicting DataLoader 0:  96%|█████████▌| 25/26 [00:00&lt;00:00, 55.34it/s]
Predicting DataLoader 0: 100%|██████████| 26/26 [00:00&lt;00:00, 57.39it/s]
Predicting DataLoader 0: 100%|██████████| 26/26 [00:00&lt;00:00, 57.32it/s]

Predicting: |          | 0/? [00:00&lt;?, ?it/s]
Predicting: |          | 0/? [00:00&lt;?, ?it/s]
Predicting DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]
Predicting DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 221.37it/s]
Predicting DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 143.57it/s]
Predicting DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 37.47it/s]
Predicting DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 49.24it/s]
Predicting DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 56.58it/s]
Predicting DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 67.04it/s]
Predicting DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 66.70it/s]

Predicting: |          | 0/? [00:00&lt;?, ?it/s]
Predicting: |          | 0/? [00:00&lt;?, ?it/s]
Predicting DataLoader 0:   0%|          | 0/26 [00:00&lt;?, ?it/s]
Predicting DataLoader 0:   4%|▍         | 1/26 [00:00&lt;00:00, 53.68it/s]
Predicting DataLoader 0:   8%|▊         | 2/26 [00:00&lt;00:00, 64.96it/s]
Predicting DataLoader 0:  12%|█▏        | 3/26 [00:00&lt;00:00, 28.96it/s]
Predicting DataLoader 0:  15%|█▌        | 4/26 [00:00&lt;00:00, 38.12it/s]
Predicting DataLoader 0:  19%|█▉        | 5/26 [00:00&lt;00:00, 38.85it/s]
Predicting DataLoader 0:  23%|██▎       | 6/26 [00:00&lt;00:00, 39.32it/s]
Predicting DataLoader 0:  27%|██▋       | 7/26 [00:00&lt;00:00, 45.50it/s]
Predicting DataLoader 0:  31%|███       | 8/26 [00:00&lt;00:00, 49.47it/s]
Predicting DataLoader 0:  35%|███▍      | 9/26 [00:00&lt;00:00, 52.72it/s]
Predicting DataLoader 0:  38%|███▊      | 10/26 [00:00&lt;00:00, 47.94it/s]
Predicting DataLoader 0:  42%|████▏     | 11/26 [00:00&lt;00:00, 50.12it/s]
Predicting DataLoader 0:  46%|████▌     | 12/26 [00:00&lt;00:00, 37.43it/s]
Predicting DataLoader 0:  50%|█████     | 13/26 [00:00&lt;00:00, 33.27it/s]
Predicting DataLoader 0:  54%|█████▍    | 14/26 [00:00&lt;00:00, 35.20it/s]
Predicting DataLoader 0:  58%|█████▊    | 15/26 [00:00&lt;00:00, 36.52it/s]
Predicting DataLoader 0:  62%|██████▏   | 16/26 [00:00&lt;00:00, 38.80it/s]
Predicting DataLoader 0:  65%|██████▌   | 17/26 [00:00&lt;00:00, 40.90it/s]
Predicting DataLoader 0:  69%|██████▉   | 18/26 [00:00&lt;00:00, 42.70it/s]
Predicting DataLoader 0:  73%|███████▎  | 19/26 [00:00&lt;00:00, 44.85it/s]
Predicting DataLoader 0:  77%|███████▋  | 20/26 [00:00&lt;00:00, 46.64it/s]
Predicting DataLoader 0:  81%|████████  | 21/26 [00:00&lt;00:00, 48.86it/s]
Predicting DataLoader 0:  85%|████████▍ | 22/26 [00:00&lt;00:00, 48.40it/s]
Predicting DataLoader 0:  88%|████████▊ | 23/26 [00:00&lt;00:00, 46.41it/s]
Predicting DataLoader 0:  92%|█████████▏| 24/26 [00:00&lt;00:00, 48.31it/s]
Predicting DataLoader 0:  96%|█████████▌| 25/26 [00:00&lt;00:00, 49.94it/s]
Predicting DataLoader 0: 100%|██████████| 26/26 [00:00&lt;00:00, 51.87it/s]
Predicting DataLoader 0: 100%|██████████| 26/26 [00:00&lt;00:00, 51.83it/s]

Predicting: |          | 0/? [00:00&lt;?, ?it/s]
Predicting: |          | 0/? [00:00&lt;?, ?it/s]
Predicting DataLoader 0:   0%|          | 0/6 [00:00&lt;?, ?it/s]
Predicting DataLoader 0:  17%|█▋        | 1/6 [00:00&lt;00:00, 376.27it/s]
Predicting DataLoader 0:  33%|███▎      | 2/6 [00:00&lt;00:00, 30.72it/s]
Predicting DataLoader 0:  50%|█████     | 3/6 [00:00&lt;00:00, 45.36it/s]
Predicting DataLoader 0:  67%|██████▋   | 4/6 [00:00&lt;00:00, 57.69it/s]
Predicting DataLoader 0:  83%|████████▎ | 5/6 [00:00&lt;00:00, 71.20it/s]
Predicting DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 82.28it/s]
Predicting DataLoader 0: 100%|██████████| 6/6 [00:00&lt;00:00, 81.81it/s]
</pre></div>
</div>
<p>We also extract the ages of the subjects for coloring the points in the
visualizations and for evaluating the representations on age prediction.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y_train_vbm</span></a> <span class="o">=</span> <span class="p">[</span><span class="n">y</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_vbm_train</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">samples</span></a><span class="p">]</span>
<a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y_test_vbm</span></a> <span class="o">=</span> <span class="p">[</span><span class="n">y</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_vbm_test</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">samples</span></a><span class="p">]</span>
<a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y_train_sbm</span></a> <span class="o">=</span> <span class="p">[</span><span class="n">y</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_sbm_train</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">samples</span></a><span class="p">]</span>
<a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y_test_sbm</span></a> <span class="o">=</span> <span class="p">[</span><span class="n">y</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dataloader_sbm_test</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">samples</span></a><span class="p">]</span>
</pre></div>
</div>
<p>We then apply MDS on the test set and visualize the results. The
points are colored according to the age of the subjects.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">plot_mds_side_by_side</span><span class="p">(</span><span class="n">Z_vbm</span><span class="p">,</span> <span class="n">Z_sbm</span><span class="p">,</span> <span class="n">y_vbm</span><span class="p">,</span> <span class="n">y_sbm</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Run MDS on VBM and SBM embeddings and plot side-by-side scatter</span>
<span class="sd">    plots.&quot;&quot;&quot;</span>
    <span class="n">mds</span> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS" class="sphx-glr-backref-module-sklearn-manifold sphx-glr-backref-type-py-class"><span class="n">MDS</span></a><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

    <span class="c1"># Fit-transform embeddings</span>
    <span class="n">Z_vbm_mds</span> <span class="o">=</span> <span class="n">mds</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Z_vbm</span><span class="p">)</span>
    <span class="n">Z_sbm_mds</span> <span class="o">=</span> <span class="n">mds</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Z_sbm</span><span class="p">)</span>

    <span class="c1"># Side-by-side plots</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="n">sc1</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">Z_vbm_mds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Z_vbm_mds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_vbm</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span>
    <span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;VBM - MDS projection&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Dim 1&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Dim 2&quot;</span><span class="p">)</span>
    <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.colorbar.html#matplotlib.pyplot.colorbar" title="matplotlib.pyplot.colorbar" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span></a><span class="p">(</span><span class="n">sc1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Age&quot;</span><span class="p">)</span>

    <span class="n">sc2</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">Z_sbm_mds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Z_sbm_mds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_sbm</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span>
    <span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;SBM - MDS projection&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Dim 1&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Dim 2&quot;</span><span class="p">)</span>
    <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.colorbar.html#matplotlib.pyplot.colorbar" title="matplotlib.pyplot.colorbar" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span></a><span class="p">(</span><span class="n">sc2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Age&quot;</span><span class="p">)</span>

    <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.suptitle.html#matplotlib.pyplot.suptitle" title="matplotlib.pyplot.suptitle" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span></a><span class="p">(</span><span class="s2">&quot;MDS projections of test embeddings&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html#matplotlib.pyplot.tight_layout" title="matplotlib.pyplot.tight_layout" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span></a><span class="p">()</span>
    <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="matplotlib.pyplot.show" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>


<span class="n">plot_mds_side_by_side</span><span class="p">(</span><a href="https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Z_test_vbm</span></a><span class="p">,</span> <a href="https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Z_test_sbm</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y_test_vbm</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y_test_sbm</span></a><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_yaware_openbhb_001.png" srcset="../_images/sphx_glr_plot_yaware_openbhb_001.png" alt="MDS projections of test embeddings, VBM - MDS projection, SBM - MDS projection" class = "sphx-glr-single-img"/><p>Finally, we evaluate the learned representations on age prediction using
linear regression and KNN regression. We report the mean absolute error and
the R^2 coefficient between the true and predicted ages on the test set for
each model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">evaluate_and_predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Z_train</span><span class="p">,</span> <span class="n">Z_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train model and return predictions + metrics.&quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Z_test</span><span class="p">)</span>
    <span class="n">mae</span> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error" class="sphx-glr-backref-module-sklearn-metrics sphx-glr-backref-type-py-function"><span class="n">mean_absolute_error</span></a><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score" class="sphx-glr-backref-module-sklearn-metrics sphx-glr-backref-type-py-function"><span class="n">r2_score</span></a><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">mae</span><span class="p">,</span> <span class="n">r2</span>


<span class="k">def</span><span class="w"> </span><span class="nf">plot_comparison</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">models</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">embeddings</span></a><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plot side-by-side scatter plots for each model and modality.</span>
<span class="sd">    models: dict of {name: model}</span>
<span class="sd">    embeddings: dict of {modality: (Z_train, Z_test, y_train, y_test)}</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_models</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">models</span></a><span class="p">)</span>
    <span class="n">n_modalities</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">embeddings</span></a><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span>
        <span class="n">n_models</span><span class="p">,</span>
        <span class="n">n_modalities</span><span class="p">,</span>
        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">n_models</span><span class="p">),</span>
        <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">row</span><span class="p">,</span> <span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">models</span></a><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
        <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="p">(</span><span class="n">modality</span><span class="p">,</span> <span class="p">(</span><span class="n">Z_train</span><span class="p">,</span> <span class="n">Z_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">embeddings</span></a><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="n">y_pred</span><span class="p">,</span> <span class="n">mae</span><span class="p">,</span> <span class="n">r2</span> <span class="o">=</span> <span class="n">evaluate_and_predict</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span> <span class="n">Z_train</span><span class="p">,</span> <span class="n">Z_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span>
            <span class="p">)</span>

            <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">]</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
                <span class="n">y_test</span><span class="p">,</span>
                <span class="n">y_pred</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span> <span class="k">if</span> <span class="n">modality</span> <span class="o">==</span> <span class="s2">&quot;SBM&quot;</span> <span class="k">else</span> <span class="s2">&quot;steelblue&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
                <span class="p">[</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.min.html#numpy.min" title="numpy.min" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">min</span></a><span class="p">(</span><span class="n">y_test</span><span class="p">),</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.max.html#numpy.max" title="numpy.max" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">max</span></a><span class="p">(</span><span class="n">y_test</span><span class="p">)],</span>
                <span class="p">[</span><a href="https://numpy.org/doc/stable/reference/generated/numpy.min.html#numpy.min" title="numpy.min" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">min</span></a><span class="p">(</span><span class="n">y_test</span><span class="p">),</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.max.html#numpy.max" title="numpy.max" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-function"><span class="n">np</span><span class="o">.</span><span class="n">max</span></a><span class="p">(</span><span class="n">y_test</span><span class="p">)],</span>
                <span class="s2">&quot;r--&quot;</span><span class="p">,</span>
                <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Ideal&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">modality</span><span class="si">}</span><span class="s2"> - </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="se">\n</span><span class="s2">MAE=</span><span class="si">{</span><span class="n">mae</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, R²=</span><span class="si">{</span><span class="n">r2</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;True Age&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">col</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Predicted Age&quot;</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

    <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.suptitle.html#matplotlib.pyplot.suptitle" title="matplotlib.pyplot.suptitle" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span></a><span class="p">(</span><span class="s2">&quot;Model Comparison: VBM vs SBM&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
    <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html#matplotlib.pyplot.tight_layout" title="matplotlib.pyplot.tight_layout" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span></a><span class="p">()</span>
    <a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" title="matplotlib.pyplot.show" class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>


<span class="c1"># Define models and embeddings</span>
<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">models</span></a> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Linear Regression&quot;</span><span class="p">:</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression" class="sphx-glr-backref-module-sklearn-linear_model sphx-glr-backref-type-py-class"><span class="n">LinearRegression</span></a><span class="p">(),</span>
    <span class="s2">&quot;KNN (k=5)&quot;</span><span class="p">:</span> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor" title="sklearn.neighbors.KNeighborsRegressor" class="sphx-glr-backref-module-sklearn-neighbors sphx-glr-backref-type-py-class"><span class="n">KNeighborsRegressor</span></a><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
<span class="p">}</span>

<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">embeddings</span></a> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;VBM&quot;</span><span class="p">:</span> <span class="p">(</span><a href="https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Z_train_vbm</span></a><span class="p">,</span> <a href="https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Z_test_vbm</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y_train_vbm</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y_test_vbm</span></a><span class="p">),</span>
    <span class="s2">&quot;SBM&quot;</span><span class="p">:</span> <span class="p">(</span><a href="https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Z_train_sbm</span></a><span class="p">,</span> <a href="https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Z_test_sbm</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y_train_sbm</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">y_test_sbm</span></a><span class="p">),</span>
<span class="p">}</span>

<span class="c1"># Run comparison</span>
<span class="n">plot_comparison</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">models</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">embeddings</span></a><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_plot_yaware_openbhb_002.png" srcset="../_images/sphx_glr_plot_yaware_openbhb_002.png" alt="Model Comparison: VBM vs SBM, VBM - Linear Regression MAE=5.62, R²=0.66, SBM - Linear Regression MAE=7.51, R²=0.38, VBM - KNN (k=5) MAE=4.98, R²=0.64, SBM - KNN (k=5) MAE=7.19, R²=0.29" class = "sphx-glr-single-img"/><p><strong>Observations</strong>: From the MDS visualizations, we can observe that both VBM
and SBM embeddings show a gradient of ages, indicating that the models have
learned to organize the data in a way that reflects age similarity. However,
the VBM embeddings appear to have a more continuous distribution of ages
compared to SBM. This suggests that VBM may capture age-related features
more effectively than SBM in this context. This is confirmed when looking at
the age prediction results, where VBM outperforms SBM for both linear
regression and KNN regression. However, the results can be improved by
working with the original 3d brain scans instead of the ROI-averaged data.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (1 minutes 25.100 seconds)</p>
<p><strong>Estimated memory usage:</strong>  110 MB</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-examples-plot-yaware-openbhb-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/3dc10726b0eb16489a8589608875ae90/plot_yaware_openbhb.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_yaware_openbhb.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/46ba3ac115ee8342b83d278cd506574a/plot_yaware_openbhb.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_yaware_openbhb.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/f5c3fd7129e6560092fd1a465674f1cc/plot_yaware_openbhb.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">plot_yaware_openbhb.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../user_guide.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">User guide</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="plot_barlowtwins_openbhb.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Self-Supervised Learning with Barlow Twins</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; The nidl developers
- Code and documentation distributed under CeCILL-B license.
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link fa-brands fa-solid fa-github fa-2x" href="https://github.com/neurospin-deepinsight/nidl" aria-label="GitHub"></a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Weakly Supervised Contrastive Learning with y-Aware</a><ul>
<li><a class="reference internal" href="#setup">Setup</a></li>
<li><a class="reference internal" href="#openbhb-datasets-and-data-augmentations-for-contrastive-learning">OpenBHB datasets and data augmentations for Contrastive Learning</a></li>
<li><a class="reference internal" href="#training-of-y-aware-contrastive-learning-models">Training of y-Aware Contrastive Learning models</a></li>
<li><a class="reference internal" href="#visualization-and-evaluation-of-the-learned-representations">Visualization and evaluation of the learned representations</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=4ea706d9"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    </body>
</html>